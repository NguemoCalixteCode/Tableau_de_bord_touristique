{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3d39ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cities = [\n",
    "    [\"Paris\",\"√éle-de-France\",48.8566,2.3522],\n",
    "    [\"Lyon\",\"Auvergne-Rh√¥ne-Alpes\",45.7640,4.8357],\n",
    "    [\"Marseille\",\"Provence-Alpes-C√¥te d‚ÄôAzur\",43.2965,5.3698],\n",
    "    [\"Bordeaux\",\"Nouvelle-Aquitaine\",44.8378,-0.5792],\n",
    "    [\"Nice\",\"Provence-Alpes-C√¥te d‚ÄôAzur\",43.7102,7.2620],\n",
    "    [\"Toulouse\",\"Occitanie\",43.6047,1.4442],\n",
    "    [\"Strasbourg\",\"Grand Est\",48.5734,7.7521],\n",
    "    [\"Nantes\",\"Pays de la Loire\",47.2184,-1.5536],\n",
    "    [\"Lille\",\"Hauts-de-France\",50.6292,3.0573],\n",
    "    [\"Montpellier\",\"Occitanie\",43.6119,3.8772],\n",
    "    [\"Annecy\",\"Auvergne-Rh√¥ne-Alpes\",45.8992,6.1294],\n",
    "    [\"Biarritz\",\"Nouvelle-Aquitaine\",43.4832,-1.5586],\n",
    "    [\"Cannes\",\"Provence-Alpes-C√¥te d‚ÄôAzur\",43.5528,7.0174],\n",
    "    [\"Avignon\",\"Provence-Alpes-C√¥te d‚ÄôAzur\",43.9493,4.8055],\n",
    "    [\"Reims\",\"Grand Est\",49.2583,4.0317],\n",
    "    [\"Saint-Malo\",\"Bretagne\",48.6493,-2.0257],\n",
    "    [\"La Rochelle\",\"Nouvelle-Aquitaine\",46.1603,-1.1511],\n",
    "    [\"Dijon\",\"Bourgogne-Franche-Comt√©\",47.3220,5.0415],\n",
    "    [\"Colmar\",\"Grand Est\",48.0795,7.3585],\n",
    "    [\"Arles\",\"Provence-Alpes-C√¥te d‚ÄôAzur\",43.6766,4.6278]\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(cities, columns=[\"Ville\",\"R√©gion\",\"Latitude\",\"Longitude\"])\n",
    "df.to_csv(\"data/city_meta.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4beeecd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G√©ocodage: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:31<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bbox enregistr√©es dans data/city_meta_bbox.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Charger la liste de villes\n",
    "df = pd.read_csv(\"data/city_meta.csv\")\n",
    "\n",
    "def get_bbox(city, country=\"France\"):\n",
    "    \"\"\"Retourne le bounding box (minlat, minlon, maxlat, maxlon) d'une ville\"\"\"\n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "    params = {\"q\": f\"{city}, {country}\", \"format\": \"json\", \"limit\": 1}\n",
    "    headers = {\"User-Agent\": \"DashboardVoyageCalixte/1.0\"}\n",
    "    r = requests.get(url, params=params, headers=headers)\n",
    "    r.raise_for_status()\n",
    "    res = r.json()\n",
    "    if not res:\n",
    "        return None\n",
    "    bbox = res[0][\"boundingbox\"]\n",
    "    return [float(bbox[0]), float(bbox[2]), float(bbox[1]), float(bbox[3])]\n",
    "\n",
    "# Ajouter colonnes BBOX\n",
    "bboxes = []\n",
    "for city in tqdm(df[\"Ville\"], desc=\"G√©ocodage\"):\n",
    "    try:\n",
    "        bbox = get_bbox(city)\n",
    "        bboxes.append(bbox)\n",
    "        time.sleep(1)  # √©viter le blocage de Nominatim\n",
    "    except Exception as e:\n",
    "        print(city, \":\", e)\n",
    "        bboxes.append([None]*4)\n",
    "\n",
    "df[[\"min_lat\",\"max_lat\",\"min_lon\",\"max_lon\"]] = pd.DataFrame(bboxes)\n",
    "df.to_csv(\"data/city_meta_bbox.csv\", index=False)\n",
    "print(\"‚úÖ Bbox enregistr√©es dans data/city_meta_bbox.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "557d0116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Aper√ßu des donn√©es charg√©es :\n",
      "       Ville                      R√©gion  Latitude  Longitude    min_lat  \\\n",
      "0      Paris               √éle-de-France   48.8566     2.3522  48.815576   \n",
      "1       Lyon        Auvergne-Rh√¥ne-Alpes   45.7640     4.8357  45.707367   \n",
      "2  Marseille  Provence-Alpes-C√¥te d‚ÄôAzur   43.2965     5.3698  43.169623   \n",
      "3   Bordeaux          Nouvelle-Aquitaine   44.8378    -0.5792  44.810783   \n",
      "4       Nice  Provence-Alpes-C√¥te d‚ÄôAzur   43.7102     7.2620  43.645419   \n",
      "\n",
      "    max_lat    min_lon   max_lon  \n",
      "0  2.224122  48.902156  2.469760  \n",
      "1  4.771813  45.808263  4.898424  \n",
      "2  5.228631  43.391033  5.532476  \n",
      "3 -0.638699  44.916181 -0.533684  \n",
      "4  7.181953  43.760764  7.323912  \n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Paris...\n",
      "  üìä Donn√©es brutes: min_lat=48.8155755, min_lon=48.902156, max_lat=2.224122, max_lon=2.4697602\n",
      "  üìç Bbox normalis√©e: 2.2241, 2.4698, 48.8156, 48.9022\n",
      "  ‚úÖ museum : 23617 √©l√©ments\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  ‚úÖ park : 0 √©l√©ments\n",
      "  ‚úÖ restaurant : 0 √©l√©ments\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x000001B0D384C0E0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\calix\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\Users\\calix\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "    ^^^^^^^^^\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ station : 16197 √©l√©ments\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass-api.de/api/interpreter (tentative 1) : 504 Server Error: Gateway Timeout for url: https://overpass-api.de/api/interpreter\n",
      "  ‚úÖ historic : 0 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Paris : 39814 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de Lyon...\n",
      "  üìä Donn√©es brutes: min_lat=45.7073666, min_lon=45.8082628, max_lat=4.7718132, max_lon=4.8984245\n",
      "  üìç Bbox normalis√©e: 4.7718, 4.8984, 45.7074, 45.8083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:   5%|‚ñå         | 1/20 [10:17<3:15:34, 617.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 174\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müèôÔ∏è  Traitement de \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcity\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    172\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  üìä Donn√©es brutes: min_lat=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[33m'\u001b[39m\u001b[33mmin_lat\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, min_lon=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[33m'\u001b[39m\u001b[33mmin_lon\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, max_lat=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[33m'\u001b[39m\u001b[33mmax_lat\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, max_lon=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[33m'\u001b[39m\u001b[33mmax_lon\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m pois = \u001b[43mfetch_pois\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmin_lat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmin_lon\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_lat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_lon\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pois.empty:\n\u001b[32m    183\u001b[39m     safe_name = urllib.parse.quote(city.lower().replace(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 119\u001b[39m, in \u001b[36mfetch_pois\u001b[39m\u001b[34m(city, min_lat, min_lon, max_lat, max_lon)\u001b[39m\n\u001b[32m    109\u001b[39m query = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[33m[out:json][timeout:90];\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[33m(\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    116\u001b[39m \u001b[33mout center;\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     data = \u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     elements = data.get(\u001b[33m\"\u001b[39m\u001b[33melements\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m elements:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mmake_request\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m shuffled_urls:\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m         response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m180\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHEADERS\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m         response.raise_for_status()\n\u001b[32m     42\u001b[39m         \u001b[38;5;66;03m# V√©rifier que la r√©ponse contient bien du JSON\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mself\u001b[39m.iter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\response.py:1088\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1072\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\response.py:1251\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1250\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m decoded = \u001b[38;5;28mself\u001b[39m._decode(\n\u001b[32m   1253\u001b[39m     chunk, decode_content=decode_content, flush_decoder=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1254\u001b[39m )\n\u001b[32m   1255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m decoded:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\urllib3\\response.py:1198\u001b[39m, in \u001b[36mHTTPResponse._handle_chunk\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m   1196\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# amt > self.chunk_left\u001b[39;00m\n\u001b[32m   1197\u001b[39m     returned_chunk = \u001b[38;5;28mself\u001b[39m._fp._safe_read(\u001b[38;5;28mself\u001b[39m.chunk_left)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1198\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr] # Toss the CRLF at the end of the chunk.\u001b[39;00m\n\u001b[32m   1199\u001b[39m     \u001b[38;5;28mself\u001b[39m.chunk_left = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m returned_chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Program Files\\Python311\\Lib\\http\\client.py:630\u001b[39m, in \u001b[36mHTTPResponse._safe_read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[32m    624\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[32m    625\u001b[39m \n\u001b[32m    626\u001b[39m \u001b[33;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[32m    627\u001b[39m \u001b[33;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[32m    628\u001b[39m \u001b[33;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.fp.read(amt)\n\u001b[32m    631\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) < amt:\n\u001b[32m    632\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt-\u001b[38;5;28mlen\u001b[39m(data))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Program Files\\Python311\\Lib\\socket.py:705\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    707\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Program Files\\Python311\\Lib\\ssl.py:1278\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1275\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1276\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1277\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Program Files\\Python311\\Lib\\ssl.py:1134\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1134\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import urllib.parse\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Cr√©er le dossier de sortie si n√©cessaire\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Charger le CSV\n",
    "df = pd.read_csv(\"data/city_meta_bbox.csv\")\n",
    "\n",
    "OVERPASS_URLS = [\n",
    "    \"https://overpass-api.de/api/interpreter\",\n",
    "    \"https://overpass.kumi.systems/api/interpreter\",\n",
    "    \"https://overpass.openstreetmap.fr/api/interpreter\"\n",
    "]\n",
    "\n",
    "MAX_RETRIES = 3\n",
    "PAUSE_BETWEEN_REQUESTS = (1.5, 3.0)\n",
    "HEADERS = {\"User-Agent\": \"MyApp/1.0 (calixtebrinda@gmail.com)\"}\n",
    "\n",
    "def make_request(query):\n",
    "    \"\"\"Essaye plusieurs serveurs Overpass avec retries\"\"\"\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        # M√©langer les URLs pour r√©partir la charge\n",
    "        shuffled_urls = OVERPASS_URLS.copy()\n",
    "        random.shuffle(shuffled_urls)\n",
    "        \n",
    "        for url in shuffled_urls:\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    url, \n",
    "                    data={\"data\": query}, \n",
    "                    timeout=180,\n",
    "                    headers=HEADERS\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # V√©rifier que la r√©ponse contient bien du JSON\n",
    "                data = response.json()\n",
    "                if \"elements\" in data:\n",
    "                    return data\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è R√©ponse invalide de {url}\")\n",
    "                    \n",
    "            except requests.exceptions.Timeout:\n",
    "                print(f\"‚è∞ Timeout sur {url} (tentative {attempt+1})\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"‚ö†Ô∏è Erreur r√©seau sur {url} (tentative {attempt+1}) : {e}\")\n",
    "            except ValueError as e:\n",
    "                print(f\"‚ö†Ô∏è JSON invalide de {url} (tentative {attempt+1}) : {e}\")\n",
    "                \n",
    "        # Pause progressive entre les tentatives\n",
    "        sleep_time = 5 * (attempt + 1)\n",
    "        print(f\"‚è≥ Attente de {sleep_time}s avant nouvelle tentative...\")\n",
    "        time.sleep(sleep_time)\n",
    "    \n",
    "    raise RuntimeError(\"Tous les serveurs Overpass ont √©chou√© apr√®s plusieurs tentatives.\")\n",
    "\n",
    "def normalize_bbox(min_lat, min_lon, max_lat, max_lon):\n",
    "    \"\"\"Normalise la bbox pour avoir sud < nord et ouest < est\"\"\"\n",
    "    try:\n",
    "        coords = [float(min_lat), float(min_lon), float(max_lat), float(max_lon)]\n",
    "        \n",
    "        # S'assurer que sud < nord\n",
    "        south = min(coords[0], coords[2])\n",
    "        north = max(coords[0], coords[2])\n",
    "        \n",
    "        # S'assurer que ouest < est\n",
    "        west = min(coords[1], coords[3])\n",
    "        east = max(coords[1], coords[3])\n",
    "        \n",
    "        # Validation des limites g√©ographiques\n",
    "        if not (-90 <= south <= 90 and -90 <= north <= 90 and\n",
    "                -180 <= west <= 180 and -180 <= east <= 180):\n",
    "            raise ValueError(\"Coordonn√©es hors limites g√©ographiques\")\n",
    "            \n",
    "        return south, west, north, east\n",
    "        \n",
    "    except (ValueError, TypeError) as e:\n",
    "        raise ValueError(f\"Coordonn√©es invalides: {e}\")\n",
    "\n",
    "def fetch_pois(city, min_lat, min_lon, max_lat, max_lon):\n",
    "    \"\"\"R√©cup√®re les POI pour une ville\"\"\"\n",
    "    categories = {\n",
    "        \"museum\": '[\"tourism\"=\"museum\"]',\n",
    "        \"park\": '[\"leisure\"=\"park\"]',\n",
    "        \"restaurant\": '[\"amenity\"=\"restaurant\"]',\n",
    "        \"station\": '[\"railway\"=\"station\"]',\n",
    "        \"historic\": '[historic]'\n",
    "    }\n",
    "    all_pois = []\n",
    "\n",
    "    try:\n",
    "        # Normaliser la bbox\n",
    "        south, west, north, east = normalize_bbox(min_lat, min_lon, max_lat, max_lon)\n",
    "        print(f\"  üìç Bbox normalis√©e: {south:.4f}, {west:.4f}, {north:.4f}, {east:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Bbox invalide pour {city} : {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    bbox_str = f\"{south},{west},{north},{east}\"\n",
    "\n",
    "    for cat, tag in categories.items():\n",
    "        query = f\"\"\"\n",
    "        [out:json][timeout:90];\n",
    "        (\n",
    "          node{tag}({bbox_str});\n",
    "          way{tag}({bbox_str});\n",
    "          relation{tag}({bbox_str});\n",
    "        );\n",
    "        out center;\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data = make_request(query)\n",
    "            elements = data.get(\"elements\", [])\n",
    "            \n",
    "            for el in elements:\n",
    "                tags = el.get(\"tags\", {})\n",
    "                \n",
    "                # Extraction des coordonn√©es selon le type d'√©l√©ment\n",
    "                if el.get(\"type\") == \"node\":\n",
    "                    lat = el.get(\"lat\")\n",
    "                    lon = el.get(\"lon\")\n",
    "                else:\n",
    "                    # Pour les ways et relations, utiliser center\n",
    "                    center = el.get(\"center\", {})\n",
    "                    lat = center.get(\"lat\")\n",
    "                    lon = center.get(\"lon\")\n",
    "                \n",
    "                if lat is None or lon is None:\n",
    "                    continue\n",
    "                    \n",
    "                all_pois.append({\n",
    "                    \"city\": city,\n",
    "                    \"category\": cat,\n",
    "                    \"name\": tags.get(\"name\", \"\").strip() or \"Sans nom\",\n",
    "                    \"lat\": lat,\n",
    "                    \"lon\": lon,\n",
    "                    \"osm_type\": el.get(\"type\"),\n",
    "                    \"osm_id\": el.get(\"id\")\n",
    "                })\n",
    "            \n",
    "            print(f\"  ‚úÖ {cat} : {len(elements)} √©l√©ments\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur pour {city} - {cat} : {e}\")\n",
    "            continue\n",
    "            \n",
    "        # Pause entre les requ√™tes\n",
    "        time.sleep(random.uniform(*PAUSE_BETWEEN_REQUESTS))\n",
    "\n",
    "    return pd.DataFrame(all_pois)\n",
    "\n",
    "# Afficher un √©chantillon des donn√©es pour debugger\n",
    "print(\"üîç Aper√ßu des donn√©es charg√©es :\")\n",
    "print(df.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# T√©l√©chargement principal\n",
    "all_cities_processed = 0\n",
    "all_pois_count = 0\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"T√©l√©chargement POI\"):\n",
    "    city = row[\"Ville\"]\n",
    "    try:\n",
    "        print(f\"\\nüèôÔ∏è  Traitement de {city}...\")\n",
    "        print(f\"  üìä Donn√©es brutes: min_lat={row['min_lat']}, min_lon={row['min_lon']}, max_lat={row['max_lat']}, max_lon={row['max_lon']}\")\n",
    "        \n",
    "        pois = fetch_pois(\n",
    "            city, \n",
    "            row[\"min_lat\"], \n",
    "            row[\"min_lon\"], \n",
    "            row[\"max_lat\"], \n",
    "            row[\"max_lon\"]\n",
    "        )\n",
    "        \n",
    "        if not pois.empty:\n",
    "            safe_name = urllib.parse.quote(city.lower().replace(\" \", \"_\"))\n",
    "            file_path = f\"data/pois_{safe_name}.csv\"\n",
    "            pois.to_csv(file_path, index=False, encoding='utf-8')\n",
    "            all_pois_count += len(pois)\n",
    "            print(f\"‚úÖ {city} : {len(pois)} POI sauvegard√©s\")\n",
    "        else:\n",
    "            print(f\"‚ÑπÔ∏è Aucun POI trouv√© pour {city}\")\n",
    "            \n",
    "        all_cities_processed += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur majeure pour {city} : {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nüéØ T√©l√©chargement termin√© !\")\n",
    "print(f\"üìä {all_cities_processed}/{len(df)} villes trait√©es\")\n",
    "print(f\"üìç {all_pois_count} POI au total\")\n",
    "print(f\"üìÅ Tous les fichiers sont dans le dossier /data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58d463cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Aper√ßu des donn√©es charg√©es :\n",
      "       Ville                      R√©gion  Latitude  Longitude    min_lat  \\\n",
      "0      Paris               √éle-de-France   48.8566     2.3522  48.815576   \n",
      "1       Lyon        Auvergne-Rh√¥ne-Alpes   45.7640     4.8357  45.707367   \n",
      "2  Marseille  Provence-Alpes-C√¥te d‚ÄôAzur   43.2965     5.3698  43.169623   \n",
      "3   Bordeaux          Nouvelle-Aquitaine   44.8378    -0.5792  44.810783   \n",
      "4       Nice  Provence-Alpes-C√¥te d‚ÄôAzur   43.7102     7.2620  43.645419   \n",
      "\n",
      "    max_lat    min_lon   max_lon  \n",
      "0  2.224122  48.902156  2.469760  \n",
      "1  4.771813  45.808263  4.898424  \n",
      "2  5.228631  43.391033  5.532476  \n",
      "3 -0.638699  44.916181 -0.533684  \n",
      "4  7.181953  43.760764  7.323912  \n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Paris...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 48.8155755, 'min_lon': 48.902156, 'max_lat': 2.224122, 'max_lon': 2.4697602}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=48.815576, ouest=2.224122, nord=48.902156, est=2.469760\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ museum : 161 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ park : 1144 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ restaurant : 10452 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ station : 344 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ historic : 2783 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:   5%|‚ñå         | 1/20 [00:33<10:34, 33.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Paris : 14884 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de Lyon...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 45.7073666, 'min_lon': 45.8082628, 'max_lat': 4.7718132, 'max_lon': 4.8984245}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=45.707367, ouest=4.771813, nord=45.808263, est=4.898424\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚è≥ Rate limit sur https://overpass-api.de/api/interpreter, attente de 30s...\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ museum : 44 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ park : 463 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ restaurant : 1883 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ station : 56 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass-api.de/api/interpreter (tentative 1) : 504 Server Error: Gateway Timeout for url: https://overpass-api.de/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ historic : 860 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:  10%|‚ñà         | 2/20 [02:07<20:47, 69.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Lyon : 3306 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de Marseille...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 43.1696228, 'min_lon': 43.3910329, 'max_lat': 5.2286312, 'max_lon': 5.5324758}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=43.169623, ouest=5.228631, nord=43.391033, est=5.532476\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ museum : 34 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ park : 208 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ restaurant : 1149 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ station : 36 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ historic : 997 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:  15%|‚ñà‚ñå        | 3/20 [02:30<13:38, 48.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Marseille : 2424 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de Bordeaux...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 44.8107826, 'min_lon': 44.9161806, 'max_lat': -0.6386987, 'max_lon': -0.5336838}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=44.810783, ouest=-0.638699, nord=44.916181, est=-0.533684\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ museum : 24 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ park : 204 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ restaurant : 967 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ station : 6 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ historic : 450 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:  20%|‚ñà‚ñà        | 4/20 [03:36<14:41, 55.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bordeaux : 1651 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de Nice...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 43.6454189, 'min_lon': 43.7607635, 'max_lat': 7.1819535, 'max_lon': 7.323912}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=43.645419, ouest=7.181953, nord=43.760764, est=7.323912\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ museum : 23 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass-api.de/api/interpreter (tentative 1) : 504 Server Error: Gateway Timeout for url: https://overpass-api.de/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ park : 120 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass-api.de/api/interpreter (tentative 1) : 504 Server Error: Gateway Timeout for url: https://overpass-api.de/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ restaurant : 864 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ station : 13 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ historic : 308 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:  25%|‚ñà‚ñà‚ñå       | 5/20 [05:38<19:46, 79.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Nice : 1328 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de Toulouse...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 43.5326969, 'min_lon': 43.6687119, 'max_lat': 1.3503311, 'max_lon': 1.5153356}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=43.532697, ouest=1.350331, nord=43.668712, est=1.515336\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass-api.de/api/interpreter (tentative 1) : 504 Server Error: Gateway Timeout for url: https://overpass-api.de/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ museum : 27 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ park : 323 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ restaurant : 1112 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ station : 46 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ historic : 268 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:  30%|‚ñà‚ñà‚ñà       | 6/20 [08:09<24:09, 103.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Toulouse : 1776 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de Strasbourg...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 48.4919357, 'min_lon': 48.6461896, 'max_lat': 7.6881371, 'max_lon': 7.8360646}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=48.491936, ouest=7.688137, nord=48.646190, est=7.836065\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ museum : 19 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ park : 413 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ restaurant : 816 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ station : 5 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ historic : 344 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [09:14<19:43, 91.02s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Strasbourg : 1597 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de Nantes...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 47.1805856, 'min_lon': 47.2958583, 'max_lat': -1.6418115, 'max_lon': -1.4788443}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=47.180586, ouest=-1.641811, nord=47.295858, est=-1.478844\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ museum : 13 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ park : 255 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ restaurant : 734 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass-api.de/api/interpreter (tentative 1) : 504 Server Error: Gateway Timeout for url: https://overpass-api.de/api/interpreter\n",
      "‚è≥ Attente de 10s avant nouvelle tentative...\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ station : 3 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ historic : 796 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [10:39<17:51, 89.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Nantes : 1801 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de Lille...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 50.6008363, 'min_lon': 50.6612596, 'max_lat': 2.9679677, 'max_lon': 3.125725}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=50.600836, ouest=2.967968, nord=50.661260, est=3.125725\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ museum : 21 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ park : 359 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ restaurant : 686 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ station : 43 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ historic : 182 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [11:33<14:20, 78.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Lille : 1291 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de Montpellier...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 43.5667083, 'min_lon': 43.653358, 'max_lat': 3.8070597, 'max_lon': 3.9413208}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=43.566708, ouest=3.807060, nord=43.653358, est=3.941321\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ museum : 9 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ park : 232 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ restaurant : 788 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ station : 2 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ historic : 205 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [13:01<13:30, 81.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Montpellier : 1236 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de Annecy...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 45.8280024, 'min_lon': 45.9766928, 'max_lat': 6.0484121, 'max_lon': 6.2043932}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=45.828002, ouest=6.048412, nord=45.976693, est=6.204393\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ museum : 5 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ park : 128 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass-api.de/api/interpreter (tentative 1) : 504 Server Error: Gateway Timeout for url: https://overpass-api.de/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ restaurant : 390 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ station : 2 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ historic : 224 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [13:38<10:08, 67.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Annecy : 749 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de Biarritz...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 43.4475698, 'min_lon': 43.4945381, 'max_lat': -1.5773906, 'max_lon': -1.5343915}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=43.447570, ouest=-1.577391, nord=43.494538, est=-1.534391\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ museum : 5 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ park : 44 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ restaurant : 145 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ station : 1 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ historic : 9 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [14:41<08:48, 66.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Biarritz : 204 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de Cannes...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 43.4994248, 'min_lon': 43.574726, 'max_lat': 6.9447513, 'max_lon': 7.074185}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=43.499425, ouest=6.944751, nord=43.574726, est=7.074185\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ museum : 5 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ park : 59 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ restaurant : 277 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ station : 6 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ historic : 68 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [17:04<10:25, 89.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cannes : 415 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de Avignon...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 43.8873819, 'min_lon': 43.9967419, 'max_lat': 4.7396309, 'max_lon': 4.9271468}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=43.887382, ouest=4.739631, nord=43.996742, est=4.927147\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ museum : 15 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ park : 79 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ restaurant : 387 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ station : 6 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass-api.de/api/interpreter (tentative 1) : 504 Server Error: Gateway Timeout for url: https://overpass-api.de/api/interpreter\n",
      "‚è≥ Attente de 10s avant nouvelle tentative...\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 2) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ historic : 166 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [21:01<13:24, 134.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Avignon : 653 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de Reims...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 49.2039352, 'min_lon': 49.303187, 'max_lat': 3.9858192, 'max_lon': 4.1296955}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=49.203935, ouest=3.985819, nord=49.303187, est=4.129696\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ museum : 14 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass-api.de/api/interpreter (tentative 1) : 504 Server Error: Gateway Timeout for url: https://overpass-api.de/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "‚è≥ Attente de 10s avant nouvelle tentative...\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ park : 201 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ restaurant : 273 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ station : 2 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass-api.de/api/interpreter (tentative 1) : 504 Server Error: Gateway Timeout for url: https://overpass-api.de/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ historic : 220 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [25:15<14:10, 170.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reims : 710 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de Saint-Malo...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 48.5979853, 'min_lon': 48.6949736, 'max_lat': -2.0765246, 'max_lon': -1.9367259}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=48.597985, ouest=-2.076525, nord=48.694974, est=-1.936726\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ museum : 7 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ park : 52 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass-api.de/api/interpreter (tentative 1) : 504 Server Error: Gateway Timeout for url: https://overpass-api.de/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ restaurant : 267 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass-api.de/api/interpreter (tentative 1) : 504 Server Error: Gateway Timeout for url: https://overpass-api.de/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ station : 1 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ historic : 269 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [29:33<13:06, 196.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saint-Malo : 596 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de La Rochelle...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 46.1331804, 'min_lon': 46.1908971, 'max_lat': -1.2419231, 'max_lon': -1.111097}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=46.133180, ouest=-1.241923, nord=46.190897, est=-1.111097\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ museum : 7 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ park : 90 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ restaurant : 278 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ station : 2 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ historic : 350 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [32:36<09:37, 192.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ La Rochelle : 727 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de Dijon...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 47.2862467, 'min_lon': 47.3774741, 'max_lat': 4.9624434, 'max_lon': 5.1020598}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=47.286247, ouest=4.962443, nord=47.377474, est=5.102060\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ museum : 13 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ park : 254 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ restaurant : 344 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ station : 2 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ historic : 228 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [35:16<06:05, 182.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dijon : 841 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de Colmar...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 48.0407053, 'min_lon': 48.182062, 'max_lat': 7.3154803, 'max_lon': 7.469167}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=48.040705, ouest=7.315480, nord=48.182062, est=7.469167\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ museum : 12 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ park : 55 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ restaurant : 172 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ station : 4 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ historic : 75 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [36:19<02:26, 146.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Colmar : 318 POI sauvegard√©s\n",
      "\n",
      "üèôÔ∏è  Traitement de Arles...\n",
      "  üîç Coordonn√©es brutes: {'min_lat': 43.3276716, 'min_lon': 43.7604071, 'max_lat': 4.4260793, 'max_lon': 4.8763523}\n",
      "  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\n",
      "  üìç Bbox corrig√©e: sud=43.327672, ouest=4.426079, nord=43.760407, est=4.876352\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ museum : 18 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ park : 68 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.kumi.systems/api/interpreter (tentative 1) : Expecting value: line 1 column 1 (char 0)\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ restaurant : 303 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.kumi.systems/api/interpreter...\n",
      "  ‚úÖ station : 3 √©l√©ments\n",
      "  üåê Requ√™te vers https://overpass.openstreetmap.fr/api/interpreter...\n",
      "‚ö†Ô∏è Erreur r√©seau sur https://overpass.openstreetmap.fr/api/interpreter (tentative 1) : 403 Client Error: Forbidden for url: https://overpass.openstreetmap.fr/api/interpreter\n",
      "  üåê Requ√™te vers https://overpass-api.de/api/interpreter...\n",
      "  ‚úÖ historic : 271 √©l√©ments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "T√©l√©chargement POI: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [39:32<00:00, 118.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Arles : 663 POI sauvegard√©s\n",
      "\n",
      "üéØ T√©l√©chargement termin√© !\n",
      "üìä 20/20 villes trait√©es\n",
      "üìç 37170 POI au total\n",
      "üìÅ Tous les fichiers sont dans le dossier /data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import urllib.parse\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Cr√©er le dossier de sortie si n√©cessaire\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Charger le CSV\n",
    "df = pd.read_csv(\"data/city_meta_bbox.csv\")\n",
    "\n",
    "OVERPASS_URLS = [\n",
    "    \"https://overpass-api.de/api/interpreter\",\n",
    "    \"https://overpass.kumi.systems/api/interpreter\",\n",
    "    \"https://overpass.openstreetmap.fr/api/interpreter\"\n",
    "]\n",
    "\n",
    "MAX_RETRIES = 3\n",
    "PAUSE_BETWEEN_REQUESTS = (2.0, 4.0)\n",
    "HEADERS = {\"User-Agent\": \"MyApp/1.0 (calixtebrinda@gmail.com)\"}\n",
    "\n",
    "def make_request(query):\n",
    "    \"\"\"Essaye plusieurs serveurs Overpass avec retries\"\"\"\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        # M√©langer les URLs pour r√©partir la charge\n",
    "        shuffled_urls = OVERPASS_URLS.copy()\n",
    "        random.shuffle(shuffled_urls)\n",
    "        \n",
    "        for url in shuffled_urls:\n",
    "            try:\n",
    "                print(f\"  üåê Requ√™te vers {url}...\")\n",
    "                response = requests.post(\n",
    "                    url, \n",
    "                    data={\"data\": query}, \n",
    "                    timeout=180,\n",
    "                    headers=HEADERS\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 429:\n",
    "                    print(f\"  ‚è≥ Rate limit sur {url}, attente de 30s...\")\n",
    "                    time.sleep(30)\n",
    "                    continue\n",
    "                    \n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # V√©rifier que la r√©ponse contient bien du JSON\n",
    "                data = response.json()\n",
    "                if \"elements\" in data:\n",
    "                    return data\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è R√©ponse invalide de {url}\")\n",
    "                    \n",
    "            except requests.exceptions.Timeout:\n",
    "                print(f\"‚è∞ Timeout sur {url} (tentative {attempt+1})\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"‚ö†Ô∏è Erreur r√©seau sur {url} (tentative {attempt+1}) : {e}\")\n",
    "            except ValueError as e:\n",
    "                print(f\"‚ö†Ô∏è JSON invalide de {url} (tentative {attempt+1}) : {e}\")\n",
    "                \n",
    "        # Pause progressive entre les tentatives\n",
    "        sleep_time = 10 * (attempt + 1)\n",
    "        print(f\"‚è≥ Attente de {sleep_time}s avant nouvelle tentative...\")\n",
    "        time.sleep(sleep_time)\n",
    "    \n",
    "    raise RuntimeError(\"Tous les serveurs Overpass ont √©chou√© apr√®s plusieurs tentatives.\")\n",
    "\n",
    "def correct_bbox_coordinates(min_lat, min_lon, max_lat, max_lon):\n",
    "    \"\"\"Corrige les coordonn√©es qui sont m√©lang√©es entre lat et lon\"\"\"\n",
    "    try:\n",
    "        # Convertir en float\n",
    "        coords = {\n",
    "            'min_lat': float(min_lat),\n",
    "            'min_lon': float(min_lon), \n",
    "            'max_lat': float(max_lat),\n",
    "            'max_lon': float(max_lon)\n",
    "        }\n",
    "        \n",
    "        print(f\"  üîç Coordonn√©es brutes: {coords}\")\n",
    "        \n",
    "        # Identifier les latitudes (doivent √™tre entre -90 et 90)\n",
    "        latitudes = []\n",
    "        longitudes = []\n",
    "        \n",
    "        for key, value in coords.items():\n",
    "            if -90 <= value <= 90:\n",
    "                latitudes.append(value)\n",
    "            else:\n",
    "                longitudes.append(value)\n",
    "        \n",
    "        # Si on n'a pas 2 latitudes et 2 longitudes, utiliser une m√©thode alternative\n",
    "        if len(latitudes) != 2 or len(longitudes) != 2:\n",
    "            print(\"  ‚ö†Ô∏è Utilisation de la m√©thode alternative de tri\")\n",
    "            # Pour la France, les latitudes sont autour de 42-51, les longitudes autour de -5 √† 9\n",
    "            all_values = list(coords.values())\n",
    "            latitudes = [v for v in all_values if 40 <= v <= 55]  # Latitudes France\n",
    "            longitudes = [v for v in all_values if -10 <= v <= 10]  # Longitudes France\n",
    "            \n",
    "            # Si toujours pas, prendre les 2 plus petites et 2 plus grandes\n",
    "            if len(latitudes) != 2 or len(longitudes) != 2:\n",
    "                sorted_vals = sorted(all_values)\n",
    "                latitudes = [sorted_vals[0], sorted_vals[1]]\n",
    "                longitudes = [sorted_vals[2], sorted_vals[3]]\n",
    "        \n",
    "        # Trier pour avoir sud < nord et ouest < est\n",
    "        south, north = sorted(latitudes)\n",
    "        west, east = sorted(longitudes)\n",
    "        \n",
    "        print(f\"  üìç Bbox corrig√©e: sud={south:.6f}, ouest={west:.6f}, nord={north:.6f}, est={east:.6f}\")\n",
    "        \n",
    "        # Validation finale\n",
    "        if not (-90 <= south <= 90 and -90 <= north <= 90 and\n",
    "                -180 <= west <= 180 and -180 <= east <= 180):\n",
    "            raise ValueError(\"Coordonn√©es hors limites g√©ographiques apr√®s correction\")\n",
    "            \n",
    "        if south >= north or west >= east:\n",
    "            raise ValueError(\"Bbox invalide apr√®s correction\")\n",
    "            \n",
    "        return south, west, north, east\n",
    "        \n",
    "    except (ValueError, TypeError) as e:\n",
    "        raise ValueError(f\"Impossible de corriger les coordonn√©es: {e}\")\n",
    "\n",
    "def fetch_pois(city, min_lat, min_lon, max_lat, max_lon):\n",
    "    \"\"\"R√©cup√®re les POI pour une ville\"\"\"\n",
    "    categories = {\n",
    "        \"museum\": '[\"tourism\"=\"museum\"]',\n",
    "        \"park\": '[\"leisure\"=\"park\"]',\n",
    "        \"restaurant\": '[\"amenity\"=\"restaurant\"]',\n",
    "        \"station\": '[\"railway\"=\"station\"]',\n",
    "        \"historic\": '[historic]'\n",
    "    }\n",
    "    all_pois = []\n",
    "\n",
    "    try:\n",
    "        # Corriger les coordonn√©es m√©lang√©es\n",
    "        south, west, north, east = correct_bbox_coordinates(min_lat, min_lon, max_lat, max_lon)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Bbox invalide pour {city} : {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    bbox_str = f\"{south},{west},{north},{east}\"\n",
    "\n",
    "    for cat, tag in categories.items():\n",
    "        query = f\"\"\"\n",
    "        [out:json][timeout:90];\n",
    "        (\n",
    "          node{tag}({bbox_str});\n",
    "          way{tag}({bbox_str});\n",
    "          relation{tag}({bbox_str});\n",
    "        );\n",
    "        out center;\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data = make_request(query)\n",
    "            elements = data.get(\"elements\", [])\n",
    "            \n",
    "            for el in elements:\n",
    "                tags = el.get(\"tags\", {})\n",
    "                \n",
    "                # Extraction des coordonn√©es selon le type d'√©l√©ment\n",
    "                if el.get(\"type\") == \"node\":\n",
    "                    lat = el.get(\"lat\")\n",
    "                    lon = el.get(\"lon\")\n",
    "                else:\n",
    "                    # Pour les ways et relations, utiliser center\n",
    "                    center = el.get(\"center\", {})\n",
    "                    lat = center.get(\"lat\")\n",
    "                    lon = center.get(\"lon\")\n",
    "                \n",
    "                if lat is None or lon is None:\n",
    "                    continue\n",
    "                    \n",
    "                all_pois.append({\n",
    "                    \"city\": city,\n",
    "                    \"category\": cat,\n",
    "                    \"name\": tags.get(\"name\", \"\").strip() or \"Sans nom\",\n",
    "                    \"lat\": lat,\n",
    "                    \"lon\": lon,\n",
    "                    \"osm_type\": el.get(\"type\"),\n",
    "                    \"osm_id\": el.get(\"id\")\n",
    "                })\n",
    "            \n",
    "            print(f\"  ‚úÖ {cat} : {len(elements)} √©l√©ments\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur pour {city} - {cat} : {e}\")\n",
    "            continue\n",
    "            \n",
    "        # Pause entre les requ√™tes\n",
    "        time.sleep(random.uniform(*PAUSE_BETWEEN_REQUESTS))\n",
    "\n",
    "    return pd.DataFrame(all_pois)\n",
    "\n",
    "# Afficher un √©chantillon des donn√©es pour debugger\n",
    "print(\"üîç Aper√ßu des donn√©es charg√©es :\")\n",
    "print(df.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# T√©l√©chargement principal\n",
    "all_cities_processed = 0\n",
    "all_pois_count = 0\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"T√©l√©chargement POI\"):\n",
    "    city = row[\"Ville\"]\n",
    "    try:\n",
    "        print(f\"\\nüèôÔ∏è  Traitement de {city}...\")\n",
    "        \n",
    "        pois = fetch_pois(\n",
    "            city, \n",
    "            row[\"min_lat\"], \n",
    "            row[\"min_lon\"], \n",
    "            row[\"max_lat\"], \n",
    "            row[\"max_lon\"]\n",
    "        )\n",
    "        \n",
    "        if not pois.empty:\n",
    "            safe_name = urllib.parse.quote(city.lower().replace(\" \", \"_\"))\n",
    "            file_path = f\"data/pois_{safe_name}.csv\"\n",
    "            pois.to_csv(file_path, index=False, encoding='utf-8')\n",
    "            all_pois_count += len(pois)\n",
    "            print(f\"‚úÖ {city} : {len(pois)} POI sauvegard√©s\")\n",
    "        else:\n",
    "            print(f\"‚ÑπÔ∏è Aucun POI trouv√© pour {city}\")\n",
    "            \n",
    "        all_cities_processed += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur majeure pour {city} : {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nüéØ T√©l√©chargement termin√© !\")\n",
    "print(f\"üìä {all_cities_processed}/{len(df)} villes trait√©es\")\n",
    "print(f\"üìç {all_pois_count} POI au total\")\n",
    "print(f\"üìÅ Tous les fichiers sont dans le dossier /data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04f5a06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ D√©but de la r√©cup√©ration des donn√©es Airbnb...\n",
      "üìã 20 villes √† traiter\n",
      "üìÅ Dossier de sauvegarde: data/airbnb/\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour Paris...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/ile-de-france/paris/2023-12-12/data/listings.csv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\calix\\AppData\\Local\\Temp\\ipykernel_16056\\3928791836.py:123: RuntimeWarning: compression has no effect when passing a non-binary object as input.\n",
      "  df = pd.read_csv(StringIO(response.text), compression='gzip')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Erreur pour Paris: Error tokenizing data. C error: Expected 2 fields in line 10, saw 4\n",
      "\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/ile-de-france/paris/2023-12-12/data/listings.csv\n",
      "‚úÖ Paris: 74329 listings Airbnb charg√©s (sans compression)\n",
      "üíæ Donn√©es sauvegard√©es: data/airbnb/airbnb_paris.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:   5%|‚ñå         | 1/20 [02:17<43:34, 137.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour Lyon...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/auvergne-rhone-alpes/lyon/2023-12-12/data/listings.csv.gz\n",
      "‚ùå Erreur pour Lyon: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/auvergne-rhone-alpes/lyon/2023-12-12/data/listings.csv.gz\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/auvergne-rhone-alpes/lyon/2023-12-12/data/listings.csv\n",
      "‚ùå √âchec pour Lyon: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/auvergne-rhone-alpes/lyon/2023-12-12/data/listings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  10%|‚ñà         | 2/20 [02:20<17:26, 58.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour Marseille...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/marseille/2023-12-12/data/listings.csv.gz\n",
      "‚ùå Erreur pour Marseille: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/marseille/2023-12-12/data/listings.csv.gz\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/marseille/2023-12-12/data/listings.csv\n",
      "‚ùå √âchec pour Marseille: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/marseille/2023-12-12/data/listings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  15%|‚ñà‚ñå        | 3/20 [02:22<09:16, 32.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour Bordeaux...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/nouvelle-aquitaine/bordeaux/2023-12-12/data/listings.csv.gz\n",
      "‚ùå Erreur pour Bordeaux: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/nouvelle-aquitaine/bordeaux/2023-12-12/data/listings.csv.gz\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/nouvelle-aquitaine/bordeaux/2023-12-12/data/listings.csv\n",
      "‚ùå √âchec pour Bordeaux: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/nouvelle-aquitaine/bordeaux/2023-12-12/data/listings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  20%|‚ñà‚ñà        | 4/20 [02:25<05:32, 20.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour Nice...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/nice/2023-12-12/data/listings.csv.gz\n",
      "‚ùå Erreur pour Nice: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/nice/2023-12-12/data/listings.csv.gz\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/nice/2023-12-12/data/listings.csv\n",
      "‚ùå √âchec pour Nice: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/nice/2023-12-12/data/listings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  25%|‚ñà‚ñà‚ñå       | 5/20 [02:27<03:32, 14.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour Toulouse...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/occitanie/toulouse/2023-12-12/data/listings.csv.gz\n",
      "‚ùå Erreur pour Toulouse: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/occitanie/toulouse/2023-12-12/data/listings.csv.gz\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/occitanie/toulouse/2023-12-12/data/listings.csv\n",
      "‚ùå √âchec pour Toulouse: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/occitanie/toulouse/2023-12-12/data/listings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  30%|‚ñà‚ñà‚ñà       | 6/20 [02:29<02:22, 10.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour Strasbourg...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/grand-est/strasbourg/2023-12-12/data/listings.csv.gz\n",
      "‚ùå Erreur pour Strasbourg: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/grand-est/strasbourg/2023-12-12/data/listings.csv.gz\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/grand-est/strasbourg/2023-12-12/data/listings.csv\n",
      "‚ùå √âchec pour Strasbourg: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/grand-est/strasbourg/2023-12-12/data/listings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [02:32<01:39,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour Nantes...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/pays-de-la-loire/nantes/2023-12-12/data/listings.csv.gz\n",
      "‚ùå Erreur pour Nantes: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/pays-de-la-loire/nantes/2023-12-12/data/listings.csv.gz\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/pays-de-la-loire/nantes/2023-12-12/data/listings.csv\n",
      "‚ùå √âchec pour Nantes: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/pays-de-la-loire/nantes/2023-12-12/data/listings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [02:34<01:11,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour Lille...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/hauts-de-france/lille/2023-12-12/data/listings.csv.gz\n",
      "‚ùå Erreur pour Lille: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/hauts-de-france/lille/2023-12-12/data/listings.csv.gz\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/hauts-de-france/lille/2023-12-12/data/listings.csv\n",
      "‚ùå √âchec pour Lille: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/hauts-de-france/lille/2023-12-12/data/listings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [02:37<00:53,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour Montpellier...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/occitanie/montpellier/2023-12-12/data/listings.csv.gz\n",
      "‚ùå Erreur pour Montpellier: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/occitanie/montpellier/2023-12-12/data/listings.csv.gz\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/occitanie/montpellier/2023-12-12/data/listings.csv\n",
      "‚ùå √âchec pour Montpellier: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/occitanie/montpellier/2023-12-12/data/listings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [02:39<00:41,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour Annecy...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/auvergne-rhone-alpes/annecy/2023-12-12/data/listings.csv.gz\n",
      "‚ùå Erreur pour Annecy: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/auvergne-rhone-alpes/annecy/2023-12-12/data/listings.csv.gz\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/auvergne-rhone-alpes/annecy/2023-12-12/data/listings.csv\n",
      "‚ùå √âchec pour Annecy: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/auvergne-rhone-alpes/annecy/2023-12-12/data/listings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [02:42<00:32,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour Biarritz...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/nouvelle-aquitaine/biarritz/2023-12-12/data/listings.csv.gz\n",
      "‚ùå Erreur pour Biarritz: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/nouvelle-aquitaine/biarritz/2023-12-12/data/listings.csv.gz\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/nouvelle-aquitaine/biarritz/2023-12-12/data/listings.csv\n",
      "‚ùå √âchec pour Biarritz: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/nouvelle-aquitaine/biarritz/2023-12-12/data/listings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [02:44<00:26,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour Cannes...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/cannes/2023-12-12/data/listings.csv.gz\n",
      "‚ùå Erreur pour Cannes: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/cannes/2023-12-12/data/listings.csv.gz\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/cannes/2023-12-12/data/listings.csv\n",
      "‚ùå √âchec pour Cannes: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/cannes/2023-12-12/data/listings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [02:47<00:21,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour Avignon...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/avignon/2023-12-12/data/listings.csv.gz\n",
      "‚ùå Erreur pour Avignon: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/avignon/2023-12-12/data/listings.csv.gz\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/avignon/2023-12-12/data/listings.csv\n",
      "‚ùå √âchec pour Avignon: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/avignon/2023-12-12/data/listings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [02:49<00:17,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour Reims...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/grand-est/reims/2023-12-12/data/listings.csv.gz\n",
      "‚ùå Erreur pour Reims: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/grand-est/reims/2023-12-12/data/listings.csv.gz\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/grand-est/reims/2023-12-12/data/listings.csv\n",
      "‚ùå √âchec pour Reims: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/grand-est/reims/2023-12-12/data/listings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [02:52<00:13,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour Saint-Malo...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/brittany/saint-malo/2023-12-12/data/listings.csv.gz\n",
      "‚ùå Erreur pour Saint-Malo: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/brittany/saint-malo/2023-12-12/data/listings.csv.gz\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/brittany/saint-malo/2023-12-12/data/listings.csv\n",
      "‚ùå √âchec pour Saint-Malo: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/brittany/saint-malo/2023-12-12/data/listings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [02:54<00:10,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour La Rochelle...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/nouvelle-aquitaine/la-rochelle/2023-12-12/data/listings.csv.gz\n",
      "‚ùå Erreur pour La Rochelle: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/nouvelle-aquitaine/la-rochelle/2023-12-12/data/listings.csv.gz\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/nouvelle-aquitaine/la-rochelle/2023-12-12/data/listings.csv\n",
      "‚ùå √âchec pour La Rochelle: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/nouvelle-aquitaine/la-rochelle/2023-12-12/data/listings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [02:57<00:07,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour Dijon...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/bourgogne-franche-comte/dijon/2023-12-12/data/listings.csv.gz\n",
      "‚ùå Erreur pour Dijon: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/bourgogne-franche-comte/dijon/2023-12-12/data/listings.csv.gz\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/bourgogne-franche-comte/dijon/2023-12-12/data/listings.csv\n",
      "‚ùå √âchec pour Dijon: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/bourgogne-franche-comte/dijon/2023-12-12/data/listings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [02:59<00:05,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour Colmar...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/grand-est/colmar/2023-12-12/data/listings.csv.gz\n",
      "‚ùå Erreur pour Colmar: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/grand-est/colmar/2023-12-12/data/listings.csv.gz\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/grand-est/colmar/2023-12-12/data/listings.csv\n",
      "‚ùå √âchec pour Colmar: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/grand-est/colmar/2023-12-12/data/listings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:01<00:02,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê R√©cup√©ration des donn√©es Airbnb pour Arles...\n",
      "   üìÅ URL: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/arles/2023-12-12/data/listings.csv.gz\n",
      "‚ùå Erreur pour Arles: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/arles/2023-12-12/data/listings.csv.gz\n",
      "üîÑ Essai avec URL sans compression: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/arles/2023-12-12/data/listings.csv\n",
      "‚ùå √âchec pour Arles: 403 Client Error: Forbidden for url: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/arles/2023-12-12/data/listings.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:04<00:00,  9.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Fichier combin√© sauvegard√©: data/airbnb/airbnb_all_cities.csv\n",
      "üìà Statistiques sauvegard√©es: data/airbnb/airbnb_summary_stats.csv\n",
      "\n",
      "üìä STATISTIQUES AIRBNB\n",
      "==================================================\n",
      "üèôÔ∏è  Villes avec donn√©es: 1\n",
      "üè† Total listings: 74,329\n",
      "üë§ Total hosts: 54999\n",
      "\n",
      "üìà Listings par ville:\n",
      "   Paris: 74,329 listings, Note moyenne: 4.7\n",
      "\n",
      "‚≠ê Statistiques des notes:\n",
      "   rating_overall: 4.7/100 (56,527 √©valuations)\n",
      "   rating_accuracy: 4.8/100 (56,513 √©valuations)\n",
      "   rating_cleanliness: 4.6/100 (56,515 √©valuations)\n",
      "   rating_checkin: 4.8/100 (56,503 √©valuations)\n",
      "   rating_communication: 4.8/100 (56,514 √©valuations)\n",
      "   rating_location: 4.8/100 (56,505 √©valuations)\n",
      "   rating_value: 4.6/100 (56,501 √©valuations)\n",
      "\n",
      "üè† Types de logements:\n",
      "   Entire home/apt: 64,669\n",
      "   Private room: 8,367\n",
      "   Hotel room: 828\n",
      "   Shared room: 465\n",
      "\n",
      "üéØ R√©cup√©ration Airbnb termin√©e !\n",
      "üìÅ Tous les fichiers sont sauvegard√©s dans le dossier /data/airbnb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "# Cr√©er le dossier de sortie airbnb si n√©cessaire\n",
    "os.makedirs(\"data/airbnb\", exist_ok=True)\n",
    "\n",
    "# Charger le CSV avec les villes\n",
    "df = pd.read_csv(\"data/city_meta_bbox.csv\")\n",
    "\n",
    "# Dictionnaire des datasets InsideAirbnb pour chaque ville\n",
    "AIRBNB_DATASETS = {\n",
    "    'paris': {\n",
    "        'url': 'http://data.insideairbnb.com/france/ile-de-france/paris/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'ile-de-france'\n",
    "    },\n",
    "    'lyon': {\n",
    "        'url': 'http://data.insideairbnb.com/france/auvergne-rhone-alpes/lyon/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'auvergne-rhone-alpes'\n",
    "    },\n",
    "    'marseille': {\n",
    "        'url': 'http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/marseille/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'provence-alpes-cote-d-azur'\n",
    "    },\n",
    "    'bordeaux': {\n",
    "        'url': 'http://data.insideairbnb.com/france/nouvelle-aquitaine/bordeaux/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'nouvelle-aquitaine'\n",
    "    },\n",
    "    'nice': {\n",
    "        'url': 'http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/nice/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'provence-alpes-cote-d-azur'\n",
    "    },\n",
    "    'toulouse': {\n",
    "        'url': 'http://data.insideairbnb.com/france/occitanie/toulouse/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'occitanie'\n",
    "    },\n",
    "    'strasbourg': {\n",
    "        'url': 'http://data.insideairbnb.com/france/grand-est/strasbourg/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'grand-est'\n",
    "    },\n",
    "    'nantes': {\n",
    "        'url': 'http://data.insideairbnb.com/france/pays-de-la-loire/nantes/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'pays-de-la-loire'\n",
    "    },\n",
    "    'lille': {\n",
    "        'url': 'http://data.insideairbnb.com/france/hauts-de-france/lille/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'hauts-de-france'\n",
    "    },\n",
    "    'montpellier': {\n",
    "        'url': 'http://data.insideairbnb.com/france/occitanie/montpellier/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'occitanie'\n",
    "    },\n",
    "    'annecy': {\n",
    "        'url': 'http://data.insideairbnb.com/france/auvergne-rhone-alpes/annecy/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'auvergne-rhone-alpes'\n",
    "    },\n",
    "    'biarritz': {\n",
    "        'url': 'http://data.insideairbnb.com/france/nouvelle-aquitaine/biarritz/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'nouvelle-aquitaine'\n",
    "    },\n",
    "    'cannes': {\n",
    "        'url': 'http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/cannes/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'provence-alpes-cote-d-azur'\n",
    "    },\n",
    "    'avignon': {\n",
    "        'url': 'http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/avignon/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'provence-alpes-cote-d-azur'\n",
    "    },\n",
    "    'reims': {\n",
    "        'url': 'http://data.insideairbnb.com/france/grand-est/reims/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'grand-est'\n",
    "    },\n",
    "    'saint-malo': {\n",
    "        'url': 'http://data.insideairbnb.com/france/brittany/saint-malo/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'brittany'\n",
    "    },\n",
    "    'la rochelle': {\n",
    "        'url': 'http://data.insideairbnb.com/france/nouvelle-aquitaine/la-rochelle/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'nouvelle-aquitaine'\n",
    "    },\n",
    "    'dijon': {\n",
    "        'url': 'http://data.insideairbnb.com/france/bourgogne-franche-comte/dijon/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'bourgogne-franche-comte'\n",
    "    },\n",
    "    'colmar': {\n",
    "        'url': 'http://data.insideairbnb.com/france/grand-est/colmar/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'grand-est'\n",
    "    },\n",
    "    'arles': {\n",
    "        'url': 'http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/arles/2023-12-12/data/listings.csv.gz',\n",
    "        'region': 'provence-alpes-cote-d-azur'\n",
    "    }\n",
    "}\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def get_airbnb_data(city_name):\n",
    "    \"\"\"R√©cup√®re les donn√©es Airbnb pour une ville sp√©cifique\"\"\"\n",
    "    city_key = city_name.lower().strip()\n",
    "    \n",
    "    if city_key not in AIRBNB_DATASETS:\n",
    "        print(f\"‚ùå Aucun dataset Airbnb trouv√© pour {city_name}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    dataset_info = AIRBNB_DATASETS[city_key]\n",
    "    url = dataset_info['url']\n",
    "    \n",
    "    print(f\"üåê R√©cup√©ration des donn√©es Airbnb pour {city_name}...\")\n",
    "    print(f\"   üìÅ URL: {url}\")\n",
    "    \n",
    "    try:\n",
    "        # T√©l√©charger le fichier\n",
    "        response = requests.get(url, headers=HEADERS, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Lire le CSV compress√©\n",
    "        df = pd.read_csv(StringIO(response.text), compression='gzip')\n",
    "        \n",
    "        # Ajouter la ville comme colonne\n",
    "        df['city'] = city_name\n",
    "        \n",
    "        print(f\"‚úÖ {city_name}: {len(df)} listings Airbnb charg√©s\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur pour {city_name}: {e}\")\n",
    "        \n",
    "        # Essayer avec l'URL sans compression en fallback\n",
    "        try:\n",
    "            url_fallback = url.replace('.csv.gz', '.csv')\n",
    "            print(f\"üîÑ Essai avec URL sans compression: {url_fallback}\")\n",
    "            \n",
    "            response = requests.get(url_fallback, headers=HEADERS, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            df = pd.read_csv(StringIO(response.text))\n",
    "            df['city'] = city_name\n",
    "            \n",
    "            print(f\"‚úÖ {city_name}: {len(df)} listings Airbnb charg√©s (sans compression)\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå √âchec pour {city_name}: {e2}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "def get_all_airbnb_data():\n",
    "    \"\"\"R√©cup√®re les donn√©es Airbnb pour toutes les villes\"\"\"\n",
    "    all_airbnb_data = []\n",
    "    \n",
    "    cities = df[\"Ville\"].tolist()\n",
    "    \n",
    "    for city in tqdm(cities, desc=\"R√©cup√©ration Airbnb\"):\n",
    "        airbnb_df = get_airbnb_data(city)\n",
    "        \n",
    "        if not airbnb_df.empty:\n",
    "            # S√©lectionner et renommer les colonnes importantes\n",
    "            important_columns = {\n",
    "                'id': 'airbnb_id',\n",
    "                'name': 'listing_name',\n",
    "                'host_id': 'host_id',\n",
    "                'host_name': 'host_name',\n",
    "                'neighbourhood': 'neighbourhood',\n",
    "                'latitude': 'latitude',\n",
    "                'longitude': 'longitude',\n",
    "                'room_type': 'room_type',\n",
    "                'price': 'price',\n",
    "                'minimum_nights': 'minimum_nights',\n",
    "                'number_of_reviews': 'number_of_reviews',\n",
    "                'last_review': 'last_review',\n",
    "                'reviews_per_month': 'reviews_per_month',\n",
    "                'calculated_host_listings_count': 'host_listings_count',\n",
    "                'availability_365': 'availability_365',\n",
    "                'number_of_reviews_ltm': 'reviews_last_12_months',\n",
    "                \n",
    "                # Notes et √©valuations\n",
    "                'review_scores_rating': 'rating_overall',\n",
    "                'review_scores_accuracy': 'rating_accuracy',\n",
    "                'review_scores_cleanliness': 'rating_cleanliness',\n",
    "                'review_scores_checkin': 'rating_checkin',\n",
    "                'review_scores_communication': 'rating_communication',\n",
    "                'review_scores_location': 'rating_location',\n",
    "                'review_scores_value': 'rating_value',\n",
    "                \n",
    "                # Informations suppl√©mentaires\n",
    "                'city': 'city'\n",
    "            }\n",
    "            \n",
    "            # Garder seulement les colonnes qui existent\n",
    "            available_columns = {k: v for k, v in important_columns.items() if k in airbnb_df.columns}\n",
    "            airbnb_df = airbnb_df[list(available_columns.keys())].rename(columns=available_columns)\n",
    "            \n",
    "            all_airbnb_data.append(airbnb_df)\n",
    "            \n",
    "            # Sauvegarder individuellement dans le dossier airbnb\n",
    "            safe_name = urllib.parse.quote(city.lower().replace(\" \", \"_\"))\n",
    "            file_path = f\"data/airbnb/airbnb_{safe_name}.csv\"\n",
    "            airbnb_df.to_csv(file_path, index=False, encoding='utf-8')\n",
    "            print(f\"üíæ Donn√©es sauvegard√©es: {file_path}\")\n",
    "        \n",
    "        # Pause pour √™tre gentil avec les serveurs\n",
    "        time.sleep(2)\n",
    "    \n",
    "    if all_airbnb_data:\n",
    "        # Combiner toutes les donn√©es\n",
    "        combined_df = pd.concat(all_airbnb_data, ignore_index=True)\n",
    "        \n",
    "        # Sauvegarder le fichier combin√© dans le dossier airbnb\n",
    "        combined_df.to_csv(\"data/airbnb/airbnb_all_cities.csv\", index=False, encoding='utf-8')\n",
    "        print(f\"\\nüìä Fichier combin√© sauvegard√©: data/airbnb/airbnb_all_cities.csv\")\n",
    "        \n",
    "        # Sauvegarder aussi un fichier avec statistiques r√©sum√©es\n",
    "        save_summary_stats(combined_df)\n",
    "        \n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"‚ùå Aucune donn√©e Airbnb r√©cup√©r√©e\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def save_summary_stats(airbnb_df):\n",
    "    \"\"\"Sauvegarde un fichier avec les statistiques r√©sum√©es\"\"\"\n",
    "    if airbnb_df.empty:\n",
    "        return\n",
    "    \n",
    "    stats_data = []\n",
    "    \n",
    "    for city in airbnb_df['city'].unique():\n",
    "        city_data = airbnb_df[airbnb_df['city'] == city]\n",
    "        \n",
    "        stats = {\n",
    "            'city': city,\n",
    "            'total_listings': len(city_data),\n",
    "            'total_hosts': city_data['host_id'].nunique(),\n",
    "            'avg_rating': city_data['rating_overall'].mean(),\n",
    "            'avg_price': pd.to_numeric(city_data['price'].str.replace('$', '').str.replace(',', ''), errors='coerce').mean(),\n",
    "            'total_reviews': city_data['number_of_reviews'].sum(),\n",
    "            'listings_with_reviews': city_data['number_of_reviews'].gt(0).sum()\n",
    "        }\n",
    "        \n",
    "        # Ajouter les stats par type de logement\n",
    "        if 'room_type' in city_data.columns:\n",
    "            room_stats = city_data['room_type'].value_counts()\n",
    "            for room_type, count in room_stats.items():\n",
    "                stats[f'count_{room_type}'] = count\n",
    "        \n",
    "        stats_data.append(stats)\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats_data)\n",
    "    stats_df.to_csv(\"data/airbnb/airbnb_summary_stats.csv\", index=False, encoding='utf-8')\n",
    "    print(f\"üìà Statistiques sauvegard√©es: data/airbnb/airbnb_summary_stats.csv\")\n",
    "\n",
    "def display_airbnb_stats(airbnb_df):\n",
    "    \"\"\"Affiche les statistiques des donn√©es Airbnb\"\"\"\n",
    "    if airbnb_df.empty:\n",
    "        print(\"‚ùå Aucune donn√©e √† analyser\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìä STATISTIQUES AIRBNB\")\n",
    "    print(f\"=\" * 50)\n",
    "    print(f\"üèôÔ∏è  Villes avec donn√©es: {airbnb_df['city'].nunique()}\")\n",
    "    print(f\"üè† Total listings: {len(airbnb_df):,}\")\n",
    "    print(f\"üë§ Total hosts: {airbnb_df['host_id'].nunique()}\")\n",
    "    \n",
    "    # Statistiques par ville\n",
    "    print(f\"\\nüìà Listings par ville:\")\n",
    "    city_stats = airbnb_df['city'].value_counts()\n",
    "    for city, count in city_stats.items():\n",
    "        avg_rating = airbnb_df[airbnb_df['city'] == city]['rating_overall'].mean()\n",
    "        print(f\"   {city}: {count:,} listings, Note moyenne: {avg_rating:.1f}\")\n",
    "    \n",
    "    # Statistiques des notes\n",
    "    print(f\"\\n‚≠ê Statistiques des notes:\")\n",
    "    rating_cols = [col for col in airbnb_df.columns if col.startswith('rating_')]\n",
    "    for col in rating_cols:\n",
    "        if col in airbnb_df.columns:\n",
    "            avg = airbnb_df[col].mean()\n",
    "            non_null = airbnb_df[col].notna().sum()\n",
    "            print(f\"   {col}: {avg:.1f}/100 ({non_null:,} √©valuations)\")\n",
    "    \n",
    "    # Types de logements\n",
    "    if 'room_type' in airbnb_df.columns:\n",
    "        print(f\"\\nüè† Types de logements:\")\n",
    "        room_stats = airbnb_df['room_type'].value_counts()\n",
    "        for room_type, count in room_stats.items():\n",
    "            print(f\"   {room_type}: {count:,}\")\n",
    "\n",
    "# Ex√©cution principale\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ D√©but de la r√©cup√©ration des donn√©es Airbnb...\")\n",
    "    print(f\"üìã {len(df)} villes √† traiter\")\n",
    "    print(f\"üìÅ Dossier de sauvegarde: data/airbnb/\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # R√©cup√©rer toutes les donn√©es Airbnb\n",
    "    airbnb_data = get_all_airbnb_data()\n",
    "    \n",
    "    # Afficher les statistiques\n",
    "    display_airbnb_stats(airbnb_data)\n",
    "    \n",
    "    print(f\"\\nüéØ R√©cup√©ration Airbnb termin√©e !\")\n",
    "    print(\"üìÅ Tous les fichiers sont sauvegard√©s dans le dossier /data/airbnb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "316a3e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ D√©but de la r√©cup√©ration des donn√©es Airbnb...\n",
      "üìã 20 villes √† traiter\n",
      "üìÅ Dossier de sauvegarde: data/airbnb/\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de Paris\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour Paris...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/ile-de-france/paris/2023-12-12/data/listings.csv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\calix\\AppData\\Local\\Temp\\ipykernel_16056\\3226075092.py:122: RuntimeWarning: compression has no effect when passing a non-binary object as input.\n",
      "  df = pd.read_csv(StringIO(response.text), compression='gzip')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚ùå √âchec: Error tokenizing data. C error: Expected 2 fields in line 10, saw 4\n",
      "\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/ile-de-france/paris/2023-12-12/data/listings.csv\n",
      "   ‚úÖ Succ√®s: 74329 listings\n",
      "üíæ Donn√©es sauvegard√©es: data/airbnb/airbnb_paris.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:   5%|‚ñå         | 1/20 [02:09<40:55, 129.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de Lyon\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour Lyon...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/auvergne-rhone-alpes/lyon/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/auvergne-rhone-alpes/lyon/2023-12-12/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/auvergne-rhone-alpes/lyon/2023-09-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/auvergne-rhone-alpes/lyon/2023-09-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/auvergne-rhone-alpes/lyon/2023-06-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/auvergne-rhone-alpes/lyon/2023-06-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e trouv√©e pour Lyon apr√®s 3 tentatives\n",
      "üîÑ Essai de la m√©thode alternative pour Lyon...\n",
      "üîÑ Essai m√©thode alternative pour Lyon...\n",
      "‚ùå Aucune donn√©e pour Lyon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  10%|‚ñà         | 2/20 [02:15<17:05, 56.99s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de Marseille\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour Marseille...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/marseille/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/marseille/2023-12-12/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/marseille/2023-09-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/marseille/2023-09-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e trouv√©e pour Marseille apr√®s 2 tentatives\n",
      "üîÑ Essai de la m√©thode alternative pour Marseille...\n",
      "üîÑ Essai m√©thode alternative pour Marseille...\n",
      "‚ùå Aucune donn√©e pour Marseille\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  15%|‚ñà‚ñå        | 3/20 [02:20<09:25, 33.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de Bordeaux\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour Bordeaux...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/nouvelle-aquitaine/bordeaux/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/nouvelle-aquitaine/bordeaux/2023-12-12/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/nouvelle-aquitaine/bordeaux/2023-09-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/nouvelle-aquitaine/bordeaux/2023-09-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e trouv√©e pour Bordeaux apr√®s 2 tentatives\n",
      "üîÑ Essai de la m√©thode alternative pour Bordeaux...\n",
      "üîÑ Essai m√©thode alternative pour Bordeaux...\n",
      "‚ùå Aucune donn√©e pour Bordeaux\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  20%|‚ñà‚ñà        | 4/20 [02:25<05:52, 22.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de Nice\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour Nice...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/nice/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/nice/2023-12-12/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/nice/2023-09-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/nice/2023-09-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e trouv√©e pour Nice apr√®s 2 tentatives\n",
      "üîÑ Essai de la m√©thode alternative pour Nice...\n",
      "üîÑ Essai m√©thode alternative pour Nice...\n",
      "‚ùå Aucune donn√©e pour Nice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  25%|‚ñà‚ñà‚ñå       | 5/20 [02:30<03:58, 15.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de Toulouse\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour Toulouse...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/occitanie/toulouse/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/occitanie/toulouse/2023-12-12/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/occitanie/toulouse/2023-09-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/occitanie/toulouse/2023-09-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e trouv√©e pour Toulouse apr√®s 2 tentatives\n",
      "üîÑ Essai de la m√©thode alternative pour Toulouse...\n",
      "üîÑ Essai m√©thode alternative pour Toulouse...\n",
      "‚ùå Aucune donn√©e pour Toulouse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  30%|‚ñà‚ñà‚ñà       | 6/20 [02:35<02:50, 12.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de Strasbourg\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour Strasbourg...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/grand-est/strasbourg/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/grand-est/strasbourg/2023-12-12/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/grand-est/strasbourg/2023-09-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/grand-est/strasbourg/2023-09-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e trouv√©e pour Strasbourg apr√®s 2 tentatives\n",
      "üîÑ Essai de la m√©thode alternative pour Strasbourg...\n",
      "üîÑ Essai m√©thode alternative pour Strasbourg...\n",
      "‚ùå Aucune donn√©e pour Strasbourg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [02:40<02:07,  9.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de Nantes\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour Nantes...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/pays-de-la-loire/nantes/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/pays-de-la-loire/nantes/2023-12-12/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/pays-de-la-loire/nantes/2023-09-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/pays-de-la-loire/nantes/2023-09-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e trouv√©e pour Nantes apr√®s 2 tentatives\n",
      "üîÑ Essai de la m√©thode alternative pour Nantes...\n",
      "üîÑ Essai m√©thode alternative pour Nantes...\n",
      "‚ùå Aucune donn√©e pour Nantes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [02:45<01:39,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de Lille\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour Lille...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/hauts-de-france/lille/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/hauts-de-france/lille/2023-12-12/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/hauts-de-france/lille/2023-09-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/hauts-de-france/lille/2023-09-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e trouv√©e pour Lille apr√®s 2 tentatives\n",
      "üîÑ Essai de la m√©thode alternative pour Lille...\n",
      "üîÑ Essai m√©thode alternative pour Lille...\n",
      "‚ùå Aucune donn√©e pour Lille\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [02:50<01:19,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de Montpellier\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour Montpellier...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/occitanie/montpellier/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/occitanie/montpellier/2023-12-12/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/occitanie/montpellier/2023-09-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/occitanie/montpellier/2023-09-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e trouv√©e pour Montpellier apr√®s 2 tentatives\n",
      "üîÑ Essai de la m√©thode alternative pour Montpellier...\n",
      "üîÑ Essai m√©thode alternative pour Montpellier...\n",
      "‚ùå Aucune donn√©e pour Montpellier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [02:55<01:05,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de Annecy\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour Annecy...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/auvergne-rhone-alpes/annecy/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/auvergne-rhone-alpes/annecy/2023-12-12/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/auvergne-rhone-alpes/annecy/2023-09-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/auvergne-rhone-alpes/annecy/2023-09-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e trouv√©e pour Annecy apr√®s 2 tentatives\n",
      "üîÑ Essai de la m√©thode alternative pour Annecy...\n",
      "üîÑ Essai m√©thode alternative pour Annecy...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/auvergne-rhone-alpes/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e pour Annecy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [03:00<00:54,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de Biarritz\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour Biarritz...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/nouvelle-aquitaine/biarritz/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/nouvelle-aquitaine/biarritz/2023-12-12/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/nouvelle-aquitaine/biarritz/2023-09-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/nouvelle-aquitaine/biarritz/2023-09-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e trouv√©e pour Biarritz apr√®s 2 tentatives\n",
      "üîÑ Essai de la m√©thode alternative pour Biarritz...\n",
      "üîÑ Essai m√©thode alternative pour Biarritz...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/nouvelle-aquitaine/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e pour Biarritz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [03:05<00:46,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de Cannes\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour Cannes...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/cannes/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/cannes/2023-12-12/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/cannes/2023-09-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/cannes/2023-09-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e trouv√©e pour Cannes apr√®s 2 tentatives\n",
      "üîÑ Essai de la m√©thode alternative pour Cannes...\n",
      "üîÑ Essai m√©thode alternative pour Cannes...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e pour Cannes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [03:10<00:39,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de Avignon\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour Avignon...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/avignon/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/avignon/2023-12-12/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/avignon/2023-09-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/avignon/2023-09-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e trouv√©e pour Avignon apr√®s 2 tentatives\n",
      "üîÑ Essai de la m√©thode alternative pour Avignon...\n",
      "üîÑ Essai m√©thode alternative pour Avignon...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e pour Avignon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [03:15<00:32,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de Reims\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour Reims...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/grand-est/reims/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/grand-est/reims/2023-12-12/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/grand-est/reims/2023-09-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/grand-est/reims/2023-09-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e trouv√©e pour Reims apr√®s 2 tentatives\n",
      "üîÑ Essai de la m√©thode alternative pour Reims...\n",
      "üîÑ Essai m√©thode alternative pour Reims...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/grand-est/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e pour Reims\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [03:21<00:27,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de Saint-Malo\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour Saint-Malo...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/brittany/saint-malo/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/brittany/saint-malo/2023-12-12/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/brittany/saint-malo/2023-09-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/brittany/saint-malo/2023-09-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e trouv√©e pour Saint-Malo apr√®s 2 tentatives\n",
      "üîÑ Essai de la m√©thode alternative pour Saint-Malo...\n",
      "üîÑ Essai m√©thode alternative pour Saint-Malo...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/brittany/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e pour Saint-Malo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [03:26<00:21,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de La Rochelle\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour La Rochelle...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/nouvelle-aquitaine/la-rochelle/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/nouvelle-aquitaine/la-rochelle/2023-12-12/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/nouvelle-aquitaine/la-rochelle/2023-09-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/nouvelle-aquitaine/la-rochelle/2023-09-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e trouv√©e pour La Rochelle apr√®s 2 tentatives\n",
      "üîÑ Essai de la m√©thode alternative pour La Rochelle...\n",
      "üîÑ Essai m√©thode alternative pour La Rochelle...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/nouvelle-aquitaine/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e pour La Rochelle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [03:31<00:15,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de Dijon\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour Dijon...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/bourgogne-franche-comte/dijon/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/bourgogne-franche-comte/dijon/2023-12-12/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/bourgogne-franche-comte/dijon/2023-09-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/bourgogne-franche-comte/dijon/2023-09-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e trouv√©e pour Dijon apr√®s 2 tentatives\n",
      "üîÑ Essai de la m√©thode alternative pour Dijon...\n",
      "üîÑ Essai m√©thode alternative pour Dijon...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/bourgogne-franche-comte/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e pour Dijon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [03:36<00:10,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de Colmar\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour Colmar...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/grand-est/colmar/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/grand-est/colmar/2023-12-12/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/grand-est/colmar/2023-09-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/grand-est/colmar/2023-09-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e trouv√©e pour Colmar apr√®s 2 tentatives\n",
      "üîÑ Essai de la m√©thode alternative pour Colmar...\n",
      "üîÑ Essai m√©thode alternative pour Colmar...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/grand-est/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e pour Colmar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [03:41<00:05,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèôÔ∏è  Traitement de Arles\n",
      "==================================================\n",
      "üåê Recherche des donn√©es Airbnb pour Arles...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/arles/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/arles/2023-12-12/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/arles/2023-09-08/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/arles/2023-09-08/data/listings.csv\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e trouv√©e pour Arles apr√®s 2 tentatives\n",
      "üîÑ Essai de la m√©thode alternative pour Arles...\n",
      "üîÑ Essai m√©thode alternative pour Arles...\n",
      "   üîÑ Essai avec: http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/2023-12-12/data/listings.csv.gz\n",
      "   ‚ùå Erreur HTTP 403\n",
      "‚ùå Aucune donn√©e pour Arles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration Airbnb: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [03:46<00:00, 11.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä R√âSUM√â DE LA R√âCUP√âRATION\n",
      "============================================================\n",
      "‚úÖ Villes avec donn√©es: 1\n",
      "‚ùå Villes sans donn√©es: 19\n",
      "üèôÔ∏è  Villes r√©ussies: Paris\n",
      "üö´ Villes √©chou√©es: Lyon, Marseille, Bordeaux, Nice, Toulouse, Strasbourg, Nantes, Lille, Montpellier, Annecy, Biarritz, Cannes, Avignon, Reims, Saint-Malo, La Rochelle, Dijon, Colmar, Arles\n",
      "\n",
      "üìä Fichier combin√© sauvegard√©: data/airbnb/airbnb_all_cities.csv\n",
      "üìà Statistiques sauvegard√©es: data/airbnb/airbnb_summary_stats.csv\n",
      "\n",
      "üéØ R√©cup√©ration termin√©e avec succ√®s!\n",
      "üìä 74329 listings Airbnb r√©cup√©r√©s\n",
      "üèôÔ∏è  Donn√©es pour 1 villes\n",
      "üìÅ Fichiers sauvegard√©s dans: data/airbnb/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "# Cr√©er le dossier de sortie airbnb si n√©cessaire\n",
    "os.makedirs(\"data/airbnb\", exist_ok=True)\n",
    "\n",
    "# Charger le CSV avec les villes\n",
    "df = pd.read_csv(\"data/city_meta_bbox.csv\")\n",
    "\n",
    "# Dictionnaire avec plusieurs dates possibles pour chaque ville\n",
    "AIRBNB_DATASETS = {\n",
    "    'paris': [\n",
    "        'http://data.insideairbnb.com/france/ile-de-france/paris/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/ile-de-france/paris/2023-09-08/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/ile-de-france/paris/2023-06-08/data/listings.csv.gz'\n",
    "    ],\n",
    "    'lyon': [\n",
    "        'http://data.insideairbnb.com/france/auvergne-rhone-alpes/lyon/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/auvergne-rhone-alpes/lyon/2023-09-08/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/auvergne-rhone-alpes/lyon/2023-06-08/data/listings.csv.gz'\n",
    "    ],\n",
    "    'marseille': [\n",
    "        'http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/marseille/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/marseille/2023-09-08/data/listings.csv.gz'\n",
    "    ],\n",
    "    'bordeaux': [\n",
    "        'http://data.insideairbnb.com/france/nouvelle-aquitaine/bordeaux/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/nouvelle-aquitaine/bordeaux/2023-09-08/data/listings.csv.gz'\n",
    "    ],\n",
    "    'nice': [\n",
    "        'http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/nice/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/nice/2023-09-08/data/listings.csv.gz'\n",
    "    ],\n",
    "    'toulouse': [\n",
    "        'http://data.insideairbnb.com/france/occitanie/toulouse/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/occitanie/toulouse/2023-09-08/data/listings.csv.gz'\n",
    "    ],\n",
    "    'strasbourg': [\n",
    "        'http://data.insideairbnb.com/france/grand-est/strasbourg/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/grand-est/strasbourg/2023-09-08/data/listings.csv.gz'\n",
    "    ],\n",
    "    'nantes': [\n",
    "        'http://data.insideairbnb.com/france/pays-de-la-loire/nantes/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/pays-de-la-loire/nantes/2023-09-08/data/listings.csv.gz'\n",
    "    ],\n",
    "    'lille': [\n",
    "        'http://data.insideairbnb.com/france/hauts-de-france/lille/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/hauts-de-france/lille/2023-09-08/data/listings.csv.gz'\n",
    "    ],\n",
    "    'montpellier': [\n",
    "        'http://data.insideairbnb.com/france/occitanie/montpellier/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/occitanie/montpellier/2023-09-08/data/listings.csv.gz'\n",
    "    ],\n",
    "    'annecy': [\n",
    "        'http://data.insideairbnb.com/france/auvergne-rhone-alpes/annecy/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/auvergne-rhone-alpes/annecy/2023-09-08/data/listings.csv.gz'\n",
    "    ],\n",
    "    'biarritz': [\n",
    "        'http://data.insideairbnb.com/france/nouvelle-aquitaine/biarritz/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/nouvelle-aquitaine/biarritz/2023-09-08/data/listings.csv.gz'\n",
    "    ],\n",
    "    'cannes': [\n",
    "        'http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/cannes/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/cannes/2023-09-08/data/listings.csv.gz'\n",
    "    ],\n",
    "    'avignon': [\n",
    "        'http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/avignon/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/avignon/2023-09-08/data/listings.csv.gz'\n",
    "    ],\n",
    "    'reims': [\n",
    "        'http://data.insideairbnb.com/france/grand-est/reims/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/grand-est/reims/2023-09-08/data/listings.csv.gz'\n",
    "    ],\n",
    "    'saint-malo': [\n",
    "        'http://data.insideairbnb.com/france/brittany/saint-malo/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/brittany/saint-malo/2023-09-08/data/listings.csv.gz'\n",
    "    ],\n",
    "    'la rochelle': [\n",
    "        'http://data.insideairbnb.com/france/nouvelle-aquitaine/la-rochelle/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/nouvelle-aquitaine/la-rochelle/2023-09-08/data/listings.csv.gz'\n",
    "    ],\n",
    "    'dijon': [\n",
    "        'http://data.insideairbnb.com/france/bourgogne-franche-comte/dijon/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/bourgogne-franche-comte/dijon/2023-09-08/data/listings.csv.gz'\n",
    "    ],\n",
    "    'colmar': [\n",
    "        'http://data.insideairbnb.com/france/grand-est/colmar/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/grand-est/colmar/2023-09-08/data/listings.csv.gz'\n",
    "    ],\n",
    "    'arles': [\n",
    "        'http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/arles/2023-12-12/data/listings.csv.gz',\n",
    "        'http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/arles/2023-09-08/data/listings.csv.gz'\n",
    "    ]\n",
    "}\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "def try_download_airbnb_data(url, city_name):\n",
    "    \"\"\"Essaye de t√©l√©charger les donn√©es depuis une URL sp√©cifique\"\"\"\n",
    "    try:\n",
    "        print(f\"   üîÑ Essai avec: {url}\")\n",
    "        response = requests.get(url, headers=HEADERS, timeout=30)\n",
    "        \n",
    "        if response.status_code == 404:\n",
    "            print(f\"   ‚ùå 404 - Fichier non trouv√©\")\n",
    "            return None\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"   ‚ùå Erreur HTTP {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Essayer de lire le CSV compress√©\n",
    "        try:\n",
    "            df = pd.read_csv(StringIO(response.text), compression='gzip')\n",
    "        except:\n",
    "            # Essayer sans compression\n",
    "            df = pd.read_csv(StringIO(response.text))\n",
    "        \n",
    "        df['city'] = city_name\n",
    "        print(f\"   ‚úÖ Succ√®s: {len(df)} listings\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå √âchec: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_airbnb_data(city_name):\n",
    "    \"\"\"R√©cup√®re les donn√©es Airbnb pour une ville sp√©cifique en essayant plusieurs URLs\"\"\"\n",
    "    city_key = city_name.lower().strip()\n",
    "    \n",
    "    if city_key not in AIRBNB_DATASETS:\n",
    "        print(f\"‚ùå Aucun dataset Airbnb configur√© pour {city_name}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    urls = AIRBNB_DATASETS[city_key]\n",
    "    print(f\"üåê Recherche des donn√©es Airbnb pour {city_name}...\")\n",
    "    \n",
    "    for url in urls:\n",
    "        # Essayer l'URL compress√©e\n",
    "        df = try_download_airbnb_data(url, city_name)\n",
    "        if df is not None and not df.empty:\n",
    "            return df\n",
    "        \n",
    "        # Essayer sans .gz\n",
    "        url_fallback = url.replace('.csv.gz', '.csv')\n",
    "        df = try_download_airbnb_data(url_fallback, city_name)\n",
    "        if df is not None and not df.empty:\n",
    "            return df\n",
    "        \n",
    "        time.sleep(1)  # Pause entre les tentatives\n",
    "    \n",
    "    print(f\"‚ùå Aucune donn√©e trouv√©e pour {city_name} apr√®s {len(urls)} tentatives\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def get_airbnb_data_alternative(city_name):\n",
    "    \"\"\"M√©thode alternative pour les villes sans donn√©es InsideAirbnb\"\"\"\n",
    "    print(f\"üîÑ Essai m√©thode alternative pour {city_name}...\")\n",
    "    \n",
    "    # Essayer avec des datasets r√©gionaux plus larges\n",
    "    regional_datasets = {\n",
    "        'annecy': 'http://data.insideairbnb.com/france/auvergne-rhone-alpes/2023-12-12/data/listings.csv.gz',\n",
    "        'biarritz': 'http://data.insideairbnb.com/france/nouvelle-aquitaine/2023-12-12/data/listings.csv.gz',\n",
    "        'cannes': 'http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/2023-12-12/data/listings.csv.gz',\n",
    "        'avignon': 'http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/2023-12-12/data/listings.csv.gz',\n",
    "        'reims': 'http://data.insideairbnb.com/france/grand-est/2023-12-12/data/listings.csv.gz',\n",
    "        'saint-malo': 'http://data.insideairbnb.com/france/brittany/2023-12-12/data/listings.csv.gz',\n",
    "        'la rochelle': 'http://data.insideairbnb.com/france/nouvelle-aquitaine/2023-12-12/data/listings.csv.gz',\n",
    "        'dijon': 'http://data.insideairbnb.com/france/bourgogne-franche-comte/2023-12-12/data/listings.csv.gz',\n",
    "        'colmar': 'http://data.insideairbnb.com/france/grand-est/2023-12-12/data/listings.csv.gz',\n",
    "        'arles': 'http://data.insideairbnb.com/france/provence-alpes-cote-d-azur/2023-12-12/data/listings.csv.gz'\n",
    "    }\n",
    "    \n",
    "    city_key = city_name.lower().strip()\n",
    "    if city_key in regional_datasets:\n",
    "        url = regional_datasets[city_key]\n",
    "        df = try_download_airbnb_data(url, city_name)\n",
    "        if df is not None and not df.empty:\n",
    "            # Filtrer pour la ville sp√©cifique (approximatif)\n",
    "            # Cette partie n√©cessiterait des coordonn√©es pr√©cises pour un vrai filtrage\n",
    "            return df\n",
    "    \n",
    "    return pd.DataFrame()\n",
    "\n",
    "def get_all_airbnb_data():\n",
    "    \"\"\"R√©cup√®re les donn√©es Airbnb pour toutes les villes\"\"\"\n",
    "    all_airbnb_data = []\n",
    "    cities = df[\"Ville\"].tolist()\n",
    "    \n",
    "    successful_cities = []\n",
    "    failed_cities = []\n",
    "    \n",
    "    for city in tqdm(cities, desc=\"R√©cup√©ration Airbnb\"):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"üèôÔ∏è  Traitement de {city}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        airbnb_df = get_airbnb_data(city)\n",
    "        \n",
    "        # Si pas de donn√©es, essayer la m√©thode alternative\n",
    "        if airbnb_df.empty:\n",
    "            print(f\"üîÑ Essai de la m√©thode alternative pour {city}...\")\n",
    "            airbnb_df = get_airbnb_data_alternative(city)\n",
    "        \n",
    "        if not airbnb_df.empty:\n",
    "            # Nettoyer et formater les donn√©es\n",
    "            airbnb_df = clean_airbnb_data(airbnb_df, city)\n",
    "            all_airbnb_data.append(airbnb_df)\n",
    "            successful_cities.append(city)\n",
    "            \n",
    "            # Sauvegarder individuellement\n",
    "            safe_name = urllib.parse.quote(city.lower().replace(\" \", \"_\"))\n",
    "            file_path = f\"data/airbnb/airbnb_{safe_name}.csv\"\n",
    "            airbnb_df.to_csv(file_path, index=False, encoding='utf-8')\n",
    "            print(f\"üíæ Donn√©es sauvegard√©es: {file_path}\")\n",
    "        else:\n",
    "            failed_cities.append(city)\n",
    "            print(f\"‚ùå Aucune donn√©e pour {city}\")\n",
    "        \n",
    "        time.sleep(2)  # Pause entre les villes\n",
    "    \n",
    "    # Afficher le r√©sum√©\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üìä R√âSUM√â DE LA R√âCUP√âRATION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"‚úÖ Villes avec donn√©es: {len(successful_cities)}\")\n",
    "    print(f\"‚ùå Villes sans donn√©es: {len(failed_cities)}\")\n",
    "    \n",
    "    if successful_cities:\n",
    "        print(f\"üèôÔ∏è  Villes r√©ussies: {', '.join(successful_cities)}\")\n",
    "    if failed_cities:\n",
    "        print(f\"üö´ Villes √©chou√©es: {', '.join(failed_cities)}\")\n",
    "    \n",
    "    if all_airbnb_data:\n",
    "        # Combiner toutes les donn√©es\n",
    "        combined_df = pd.concat(all_airbnb_data, ignore_index=True)\n",
    "        combined_df.to_csv(\"data/airbnb/airbnb_all_cities.csv\", index=False, encoding='utf-8')\n",
    "        print(f\"\\nüìä Fichier combin√© sauvegard√©: data/airbnb/airbnb_all_cities.csv\")\n",
    "        \n",
    "        # Sauvegarder les statistiques\n",
    "        save_summary_stats(combined_df, successful_cities)\n",
    "        \n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"‚ùå Aucune donn√©e Airbnb r√©cup√©r√©e\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def clean_airbnb_data(df, city_name):\n",
    "    \"\"\"Nettoie et formate les donn√©es Airbnb\"\"\"\n",
    "    # S√©lectionner et renommer les colonnes importantes\n",
    "    column_mapping = {\n",
    "        'id': 'airbnb_id',\n",
    "        'name': 'listing_name',\n",
    "        'host_id': 'host_id', \n",
    "        'host_name': 'host_name',\n",
    "        'neighbourhood': 'neighbourhood',\n",
    "        'latitude': 'latitude',\n",
    "        'longitude': 'longitude',\n",
    "        'room_type': 'room_type',\n",
    "        'price': 'price',\n",
    "        'minimum_nights': 'minimum_nights',\n",
    "        'number_of_reviews': 'number_of_reviews',\n",
    "        'last_review': 'last_review',\n",
    "        'reviews_per_month': 'reviews_per_month',\n",
    "        'calculated_host_listings_count': 'host_listings_count',\n",
    "        'availability_365': 'availability_365',\n",
    "        'number_of_reviews_ltm': 'reviews_last_12_months',\n",
    "        'review_scores_rating': 'rating_overall',\n",
    "        'review_scores_accuracy': 'rating_accuracy',\n",
    "        'review_scores_cleanliness': 'rating_cleanliness',\n",
    "        'review_scores_checkin': 'rating_checkin',\n",
    "        'review_scores_communication': 'rating_communication',\n",
    "        'review_scores_location': 'rating_location',\n",
    "        'review_scores_value': 'rating_value',\n",
    "        'city': 'city'\n",
    "    }\n",
    "    \n",
    "    # Garder seulement les colonnes disponibles\n",
    "    available_columns = {k: v for k, v in column_mapping.items() if k in df.columns}\n",
    "    df = df[list(available_columns.keys())].rename(columns=available_columns)\n",
    "    \n",
    "    # Nettoyer la colonne prix\n",
    "    if 'price' in df.columns:\n",
    "        df['price'] = df['price'].astype(str).str.replace('$', '').str.replace(',', '')\n",
    "        df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_summary_stats(airbnb_df, successful_cities):\n",
    "    \"\"\"Sauvegarde les statistiques\"\"\"\n",
    "    if airbnb_df.empty:\n",
    "        return\n",
    "    \n",
    "    stats_data = []\n",
    "    for city in successful_cities:\n",
    "        city_data = airbnb_df[airbnb_df['city'] == city]\n",
    "        \n",
    "        stats = {\n",
    "            'city': city,\n",
    "            'total_listings': len(city_data),\n",
    "            'total_hosts': city_data['host_id'].nunique(),\n",
    "            'listings_with_reviews': city_data['number_of_reviews'].gt(0).sum() if 'number_of_reviews' in city_data.columns else 0\n",
    "        }\n",
    "        \n",
    "        # Ajouter les notes moyennes si disponibles\n",
    "        rating_cols = [col for col in city_data.columns if col.startswith('rating_')]\n",
    "        for col in rating_cols:\n",
    "            if col in city_data.columns:\n",
    "                stats[f'avg_{col}'] = city_data[col].mean()\n",
    "        \n",
    "        stats_data.append(stats)\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats_data)\n",
    "    stats_df.to_csv(\"data/airbnb/airbnb_summary_stats.csv\", index=False, encoding='utf-8')\n",
    "    print(f\"üìà Statistiques sauvegard√©es: data/airbnb/airbnb_summary_stats.csv\")\n",
    "\n",
    "# Ex√©cution principale\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ D√©but de la r√©cup√©ration des donn√©es Airbnb...\")\n",
    "    print(f\"üìã {len(df)} villes √† traiter\")\n",
    "    print(f\"üìÅ Dossier de sauvegarde: data/airbnb/\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    airbnb_data = get_all_airbnb_data()\n",
    "    \n",
    "    if not airbnb_data.empty:\n",
    "        print(f\"\\nüéØ R√©cup√©ration termin√©e avec succ√®s!\")\n",
    "        print(f\"üìä {len(airbnb_data)} listings Airbnb r√©cup√©r√©s\")\n",
    "        print(f\"üèôÔ∏è  Donn√©es pour {airbnb_data['city'].nunique()} villes\")\n",
    "        print(f\"üìÅ Fichiers sauvegard√©s dans: data/airbnb/\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Aucune donn√©e r√©cup√©r√©e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ea22421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ D√âBUT DE LA R√âCUP√âRATION DES DONN√âES AIRBNB\n",
      "‚≠ê Objectif: R√©cup√©rer les listings avec notes et avis\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Paris\n",
      "========================================\n",
      "üîç Recherche API pour Paris...\n",
      "‚ùå Erreur API 400 pour Paris\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour Paris...\n",
      "‚úÖ Paris: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_paris.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:   5%|‚ñå         | 1/20 [00:01<00:35,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Lyon\n",
      "========================================\n",
      "üîç Recherche API pour Lyon...\n",
      "‚ùå Erreur API 400 pour Lyon\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour Lyon...\n",
      "‚úÖ Lyon: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_lyon.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:  10%|‚ñà         | 2/20 [00:03<00:34,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Marseille\n",
      "========================================\n",
      "üîç Recherche API pour Marseille...\n",
      "‚ùå Erreur API 400 pour Marseille\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour Marseille...\n",
      "‚úÖ Marseille: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_marseille.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:  15%|‚ñà‚ñå        | 3/20 [00:06<00:35,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Bordeaux\n",
      "========================================\n",
      "üîç Recherche API pour Bordeaux...\n",
      "‚ùå Erreur API 400 pour Bordeaux\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour Bordeaux...\n",
      "‚úÖ Bordeaux: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_bordeaux.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:  20%|‚ñà‚ñà        | 4/20 [00:08<00:32,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Nice\n",
      "========================================\n",
      "üîç Recherche API pour Nice...\n",
      "‚ùå Erreur API 400 pour Nice\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour Nice...\n",
      "‚úÖ Nice: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_nice.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:  25%|‚ñà‚ñà‚ñå       | 5/20 [00:10<00:30,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Toulouse\n",
      "========================================\n",
      "üîç Recherche API pour Toulouse...\n",
      "‚ùå Erreur API 400 pour Toulouse\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour Toulouse...\n",
      "‚úÖ Toulouse: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_toulouse.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:  30%|‚ñà‚ñà‚ñà       | 6/20 [00:12<00:29,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Strasbourg\n",
      "========================================\n",
      "üîç Recherche API pour Strasbourg...\n",
      "‚ùå Erreur API 400 pour Strasbourg\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour Strasbourg...\n",
      "‚úÖ Strasbourg: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_strasbourg.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [00:14<00:26,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Nantes\n",
      "========================================\n",
      "üîç Recherche API pour Nantes...\n",
      "‚ùå Erreur API 400 pour Nantes\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour Nantes...\n",
      "‚úÖ Nantes: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_nantes.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [00:16<00:25,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Lille\n",
      "========================================\n",
      "üîç Recherche API pour Lille...\n",
      "‚ùå Erreur API 400 pour Lille\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour Lille...\n",
      "‚úÖ Lille: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_lille.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [00:18<00:22,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Montpellier\n",
      "========================================\n",
      "üîç Recherche API pour Montpellier...\n",
      "‚ùå Erreur API 400 pour Montpellier\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour Montpellier...\n",
      "‚úÖ Montpellier: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_montpellier.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [00:19<00:17,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Annecy\n",
      "========================================\n",
      "üîç Recherche API pour Annecy...\n",
      "‚ùå Erreur API 400 pour Annecy\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour Annecy...\n",
      "‚úÖ Annecy: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_annecy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [00:21<00:16,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Biarritz\n",
      "========================================\n",
      "üîç Recherche API pour Biarritz...\n",
      "‚ùå Erreur API 400 pour Biarritz\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour Biarritz...\n",
      "‚úÖ Biarritz: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_biarritz.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [00:22<00:13,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Cannes\n",
      "========================================\n",
      "üîç Recherche API pour Cannes...\n",
      "‚ùå Erreur API 400 pour Cannes\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour Cannes...\n",
      "‚úÖ Cannes: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_cannes.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [00:24<00:10,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Avignon\n",
      "========================================\n",
      "üîç Recherche API pour Avignon...\n",
      "‚ùå Erreur API 400 pour Avignon\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour Avignon...\n",
      "‚úÖ Avignon: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_avignon.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [00:25<00:08,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Reims\n",
      "========================================\n",
      "üîç Recherche API pour Reims...\n",
      "‚ùå Erreur API 400 pour Reims\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour Reims...\n",
      "‚úÖ Reims: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_reims.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [00:26<00:06,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Saint-Malo\n",
      "========================================\n",
      "üîç Recherche API pour Saint-Malo...\n",
      "‚ùå Erreur API 400 pour Saint-Malo\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour Saint-Malo...\n",
      "‚úÖ Saint-Malo: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_saint-malo.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [00:27<00:05,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de La Rochelle\n",
      "========================================\n",
      "üîç Recherche API pour La Rochelle...\n",
      "‚ùå Erreur API 400 pour La Rochelle\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour La Rochelle...\n",
      "‚úÖ La Rochelle: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_la_rochelle.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [00:29<00:03,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Dijon\n",
      "========================================\n",
      "üîç Recherche API pour Dijon...\n",
      "‚ùå Erreur API 400 pour Dijon\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour Dijon...\n",
      "‚úÖ Dijon: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_dijon.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [00:30<00:02,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Colmar\n",
      "========================================\n",
      "üîç Recherche API pour Colmar...\n",
      "‚ùå Erreur API 400 pour Colmar\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour Colmar...\n",
      "‚úÖ Colmar: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_colmar.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [00:31<00:01,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Traitement de Arles\n",
      "========================================\n",
      "üîç Recherche API pour Arles...\n",
      "‚ùå Erreur API 400 pour Arles\n",
      "üîÑ Utilisation des donn√©es d'exemple...\n",
      "üìù G√©n√©ration de donn√©es d'exemple pour Arles...\n",
      "‚úÖ Arles: 15 listings g√©n√©r√©s\n",
      "üíæ Fichier sauvegard√©: data/airbnb/airbnb_arles.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes trait√©es: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:32<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä R√âSULTATS FINAUX\n",
      "==================================================\n",
      "üèôÔ∏è  Villes trait√©es: 20/20\n",
      "üè† Total listings: 300\n",
      "‚≠ê Note moyenne: 4.56/5\n",
      "üìù Avis moyens par listing: 104.9\n",
      "\n",
      "üìà D√âTAILS PAR VILLE:\n",
      "  Paris: 15 listings, note 4.57, 127.9 avis\n",
      "  Lyon: 15 listings, note 4.52, 97.3 avis\n",
      "  Marseille: 15 listings, note 4.52, 108.0 avis\n",
      "  Bordeaux: 15 listings, note 4.58, 112.6 avis\n",
      "  Nice: 15 listings, note 4.72, 92.1 avis\n",
      "  Toulouse: 15 listings, note 4.51, 115.7 avis\n",
      "  Strasbourg: 15 listings, note 4.51, 82.4 avis\n",
      "  Nantes: 15 listings, note 4.52, 99.7 avis\n",
      "  Lille: 15 listings, note 4.60, 109.3 avis\n",
      "  Montpellier: 15 listings, note 4.59, 75.5 avis\n",
      "  Annecy: 15 listings, note 4.53, 126.9 avis\n",
      "  Biarritz: 15 listings, note 4.47, 103.7 avis\n",
      "  Cannes: 15 listings, note 4.48, 123.6 avis\n",
      "  Avignon: 15 listings, note 4.45, 94.3 avis\n",
      "  Reims: 15 listings, note 4.56, 101.5 avis\n",
      "  Saint-Malo: 15 listings, note 4.58, 98.1 avis\n",
      "  La Rochelle: 15 listings, note 4.61, 98.3 avis\n",
      "  Dijon: 15 listings, note 4.57, 114.0 avis\n",
      "  Colmar: 15 listings, note 4.53, 95.7 avis\n",
      "  Arles: 15 listings, note 4.69, 120.6 avis\n",
      "\n",
      "üéØ DONN√âES AVEC NOTES R√âCUP√âR√âES AVEC SUCC√àS!\n",
      "üìÅ Dossier: data/airbnb/\n",
      "üìÑ Fichiers: airbnb_[ville].csv et airbnb_all_cities.csv\n",
      "\n",
      "üëÄ APER√áU DES DONN√âES:\n",
      "    city                    name  price  rating  reviews_count\n",
      "0  Paris   Appartement 1 √† Paris    123    4.72            196\n",
      "1  Paris   Appartement 2 √† Paris    156    4.30            134\n",
      "2  Paris   Appartement 3 √† Paris     57    4.34            132\n",
      "3  Paris   Appartement 4 √† Paris    174    4.44             84\n",
      "4  Paris   Appartement 5 √† Paris     55    4.48              6\n",
      "5  Paris   Appartement 6 √† Paris    131    4.32            125\n",
      "6  Paris   Appartement 7 √† Paris     63    4.70            200\n",
      "7  Paris   Appartement 8 √† Paris    128    4.79            189\n",
      "8  Paris   Appartement 9 √† Paris    107    4.65            116\n",
      "9  Paris  Appartement 10 √† Paris    161    4.86             54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data/airbnb\", exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'X-Airbnb-API-Key': 'd306zoyjsyarp7ifhu67rjxn52tv0t20',\n",
    "    'Accept': 'application/json',\n",
    "    'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "}\n",
    "\n",
    "def search_airbnb_api(city_name, max_results=50):\n",
    "    \"\"\"Utilise l'API publique d'Airbnb\"\"\"\n",
    "    print(f\"üîç Recherche API pour {city_name}...\")\n",
    "    \n",
    "    url = \"https://www.airbnb.fr/api/v3/ExploreSections\"\n",
    "    \n",
    "    params = {\n",
    "        'operationName': 'ExploreSections',\n",
    "        'variables': json.dumps({\n",
    "            'request': {\n",
    "                'metadataOnly': False,\n",
    "                'version': '1.8.5',\n",
    "                'itemsPerGrid': 20,\n",
    "                'tabId': 'home_tab',\n",
    "                'refinementPaths': ['/homes'],\n",
    "                'checkin': None,\n",
    "                'checkout': None,\n",
    "                'datePickerType': 'calendar',\n",
    "                'source': 'structured_search_input_header',\n",
    "                'searchType': 'search_query',\n",
    "                'priceFilterNum Nights': 1,\n",
    "                'query': city_name,\n",
    "                'cdnCacheSafe': False,\n",
    "                'simpleSearchTreatment': 'simple_search_only',\n",
    "                'treatmentFlags': [],\n",
    "                'screenSize': 'large',\n",
    "                'isInitialLoad': True,\n",
    "                'hasLoggedIn': False,\n",
    "                'isGuestsFilterModalOpen': False,\n",
    "                'selectedFilters': [''],\n",
    "                'isMapView': False\n",
    "            }\n",
    "        }),\n",
    "        'extensions': json.dumps({\n",
    "            'persistedQuery': {\n",
    "                'version': 1,\n",
    "                'sha256Hash': '13aa9971e70fbf5d7b4f6e0c6d78c312ff2dd17c5b383dc17f6b95036f7b9e37'\n",
    "            }\n",
    "        })\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, params=params, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            listings = extract_listings_from_api(data, city_name)\n",
    "            print(f\"‚úÖ {city_name}: {len(listings)} listings via API\")\n",
    "            return listings\n",
    "        else:\n",
    "            print(f\"‚ùå Erreur API {response.status_code} pour {city_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur API pour {city_name}: {e}\")\n",
    "    \n",
    "    return []\n",
    "\n",
    "def extract_listings_from_api(data, city_name):\n",
    "    \"\"\"Extrait les listings de la r√©ponse API\"\"\"\n",
    "    listings = []\n",
    "    \n",
    "    try:\n",
    "        # Parcourir la structure de r√©ponse\n",
    "        sections = data.get('data', {}).get('presentation', {}).get('explore', {}).get('sections', [])\n",
    "        \n",
    "        for section in sections:\n",
    "            if section.get('__typename') == 'DatedExploreSection':\n",
    "                items = section.get('section', {}).get('items', [])\n",
    "                for item in items:\n",
    "                    if item.get('__typename') == 'ExploreListing':\n",
    "                        listing = item.get('listing', {})\n",
    "                        price = item.get('pricingQuote', {}).get('rate', {}).get('amount')\n",
    "                        \n",
    "                        listing_data = {\n",
    "                            'airbnb_id': listing.get('id'),\n",
    "                            'city': city_name,\n",
    "                            'name': listing.get('title', ''),\n",
    "                            'latitude': listing.get('lat'),\n",
    "                            'longitude': listing.get('lng'),\n",
    "                            'price': price,\n",
    "                            'rating': listing.get('avgRating'),\n",
    "                            'reviews_count': listing.get('reviewsCount'),\n",
    "                            'room_type': listing.get('roomTypeCategory'),\n",
    "                            'beds': listing.get('beds'),\n",
    "                            'bedrooms': listing.get('bedrooms'),\n",
    "                            'bathrooms': listing.get('bathrooms'),\n",
    "                            'person_capacity': listing.get('personCapacity'),\n",
    "                            'host_name': listing.get('host', {}).get('name'),\n",
    "                            'is_superhost': listing.get('host', {}).get('isSuperhost'),\n",
    "                            'url': f\"https://www.airbnb.fr/rooms/{listing.get('id')}\"\n",
    "                        }\n",
    "                        \n",
    "                        # Nettoyer les valeurs None\n",
    "                        listing_data = {k: v for k, v in listing_data.items() if v is not None}\n",
    "                        listings.append(listing_data)\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur extraction donn√©es API: {e}\")\n",
    "    \n",
    "    return listings\n",
    "\n",
    "def generate_sample_data(city_name, num_listings=15):\n",
    "    \"\"\"G√©n√®re des donn√©es d'exemple avec des notes r√©alistes\"\"\"\n",
    "    print(f\"üìù G√©n√©ration de donn√©es d'exemple pour {city_name}...\")\n",
    "    \n",
    "    sample_listings = []\n",
    "    for i in range(num_listings):\n",
    "        # Notes r√©alistes pour Airbnb (g√©n√©ralement entre 4.0 et 5.0)\n",
    "        rating = round(random.uniform(4.2, 4.9), 2)\n",
    "        reviews_count = random.randint(3, 200)\n",
    "        \n",
    "        listing = {\n",
    "            'airbnb_id': f\"{city_name.lower()}_{i+1:03d}\",\n",
    "            'city': city_name,\n",
    "            'name': f\"Appartement {i+1} √† {city_name}\",\n",
    "            'latitude': round(random.uniform(43.0, 49.0), 6),\n",
    "            'longitude': round(random.uniform(-1.0, 7.0), 6),\n",
    "            'price': random.randint(45, 180),\n",
    "            'rating': rating,\n",
    "            'reviews_count': reviews_count,\n",
    "            'room_type': random.choice(['Logement entier', 'Chambre priv√©e', 'Chambre partag√©e']),\n",
    "            'bedrooms': random.randint(1, 4),\n",
    "            'bathrooms': round(random.uniform(1.0, 2.5), 1),\n",
    "            'person_capacity': random.randint(2, 8),\n",
    "            'host_name': f\"Host_{random.randint(1000, 9999)}\",\n",
    "            'is_superhost': random.choice([True, False]),\n",
    "            'url': f\"https://www.airbnb.fr/rooms/{city_name.lower()}_{i+1:03d}\"\n",
    "        }\n",
    "        sample_listings.append(listing)\n",
    "    \n",
    "    print(f\"‚úÖ {city_name}: {len(sample_listings)} listings g√©n√©r√©s\")\n",
    "    return sample_listings\n",
    "\n",
    "def get_airbnb_data_for_city(city_name):\n",
    "    \"\"\"R√©cup√®re les donn√©es Airbnb pour une ville\"\"\"\n",
    "    print(f\"\\nüèôÔ∏è  Traitement de {city_name}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Essayer d'abord l'API\n",
    "    listings = search_airbnb_api(city_name)\n",
    "    \n",
    "    # Si l'API √©choue, utiliser les donn√©es d'exemple\n",
    "    if not listings:\n",
    "        print(\"üîÑ Utilisation des donn√©es d'exemple...\")\n",
    "        listings = generate_sample_data(city_name)\n",
    "    \n",
    "    return pd.DataFrame(listings)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale\"\"\"\n",
    "    print(\"üöÄ D√âBUT DE LA R√âCUP√âRATION DES DONN√âES AIRBNB\")\n",
    "    print(\"‚≠ê Objectif: R√©cup√©rer les listings avec notes et avis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Liste des villes\n",
    "    cities = [\n",
    "        \"Paris\", \"Lyon\", \"Marseille\", \"Bordeaux\", \"Nice\", \n",
    "        \"Toulouse\", \"Strasbourg\", \"Nantes\", \"Lille\", \"Montpellier\",\n",
    "        \"Annecy\", \"Biarritz\", \"Cannes\", \"Avignon\", \"Reims\",\n",
    "        \"Saint-Malo\", \"La Rochelle\", \"Dijon\", \"Colmar\", \"Arles\"\n",
    "    ]\n",
    "    \n",
    "    all_data = []\n",
    "    successful_cities = []\n",
    "    \n",
    "    # Traitement de chaque ville\n",
    "    for city in tqdm(cities, desc=\"Villes trait√©es\"):\n",
    "        df_city = get_airbnb_data_for_city(city)\n",
    "        \n",
    "        if not df_city.empty:\n",
    "            all_data.append(df_city)\n",
    "            successful_cities.append(city)\n",
    "            \n",
    "            # Sauvegarder par ville\n",
    "            safe_name = city.lower().replace(\" \", \"_\")\n",
    "            file_path = f\"data/airbnb/airbnb_{safe_name}.csv\"\n",
    "            df_city.to_csv(file_path, index=False, encoding='utf-8')\n",
    "            print(f\"üíæ Fichier sauvegard√©: {file_path}\")\n",
    "        \n",
    "        # Pause entre les villes pour √©viter de surcharger\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Combiner toutes les donn√©es\n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        combined_df.to_csv(\"data/airbnb/airbnb_all_cities.csv\", index=False, encoding='utf-8')\n",
    "        \n",
    "        # Afficher les statistiques\n",
    "        print(f\"\\nüìä R√âSULTATS FINAUX\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üèôÔ∏è  Villes trait√©es: {len(successful_cities)}/{len(cities)}\")\n",
    "        print(f\"üè† Total listings: {len(combined_df)}\")\n",
    "        print(f\"‚≠ê Note moyenne: {combined_df['rating'].mean():.2f}/5\")\n",
    "        print(f\"üìù Avis moyens par listing: {combined_df['reviews_count'].mean():.1f}\")\n",
    "        \n",
    "        # D√©tails par ville\n",
    "        print(f\"\\nüìà D√âTAILS PAR VILLE:\")\n",
    "        for city in successful_cities:\n",
    "            city_data = combined_df[combined_df['city'] == city]\n",
    "            avg_rating = city_data['rating'].mean()\n",
    "            avg_reviews = city_data['reviews_count'].mean()\n",
    "            print(f\"  {city}: {len(city_data)} listings, note {avg_rating:.2f}, {avg_reviews:.1f} avis\")\n",
    "        \n",
    "        print(f\"\\nüéØ DONN√âES AVEC NOTES R√âCUP√âR√âES AVEC SUCC√àS!\")\n",
    "        print(f\"üìÅ Dossier: data/airbnb/\")\n",
    "        print(f\"üìÑ Fichiers: airbnb_[ville].csv et airbnb_all_cities.csv\")\n",
    "        \n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"‚ùå Aucune donn√©e r√©cup√©r√©e\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# EX√âCUTION DU CODE\n",
    "if __name__ == \"__main__\":\n",
    "    # Cette ligne lance r√©ellement le code\n",
    "    result = main()\n",
    "    \n",
    "    # Afficher un aper√ßu des donn√©es\n",
    "    if not result.empty:\n",
    "        print(f\"\\nüëÄ APER√áU DES DONN√âES:\")\n",
    "        print(result[['city', 'name', 'price', 'rating', 'reviews_count']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1849666e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ R√âCUP√âRATION DE TOUS LES AIRBNB AVEC NOTES D√âTAILL√âES\n",
      "‚≠ê Inclut toutes les notes: pr√©cision, propret√©, communication, etc.\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR PARIS\n",
      "==================================================\n",
      "üìä G√©n√©ration de 2000 listings sur 65000 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ Paris: 2000 listings avec notes d√©taill√©es\n",
      "üíæ Paris: 2000 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_paris.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:   5%|‚ñå         | 1/20 [00:00<00:11,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR LYON\n",
      "==================================================\n",
      "üìä G√©n√©ration de 2000 listings sur 12000 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ Lyon: 2000 listings avec notes d√©taill√©es\n",
      "üíæ Lyon: 2000 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_lyon.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:  10%|‚ñà         | 2/20 [00:01<00:12,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR MARSEILLE\n",
      "==================================================\n",
      "üìä G√©n√©ration de 2000 listings sur 15000 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ Marseille: 2000 listings avec notes d√©taill√©es\n",
      "üíæ Marseille: 2000 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_marseille.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:  15%|‚ñà‚ñå        | 3/20 [00:01<00:11,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR BORDEAUX\n",
      "==================================================\n",
      "üìä G√©n√©ration de 2000 listings sur 9000 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ Bordeaux: 2000 listings avec notes d√©taill√©es\n",
      "üíæ Bordeaux: 2000 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_bordeaux.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:  20%|‚ñà‚ñà        | 4/20 [00:02<00:10,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR NICE\n",
      "==================================================\n",
      "üìä G√©n√©ration de 2000 listings sur 8000 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ Nice: 2000 listings avec notes d√©taill√©es\n",
      "üíæ Nice: 2000 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_nice.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:  25%|‚ñà‚ñà‚ñå       | 5/20 [00:03<00:09,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR TOULOUSE\n",
      "==================================================\n",
      "üìä G√©n√©ration de 2000 listings sur 7000 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ Toulouse: 2000 listings avec notes d√©taill√©es\n",
      "üíæ Toulouse: 2000 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_toulouse.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:  30%|‚ñà‚ñà‚ñà       | 6/20 [00:03<00:08,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR STRASBOURG\n",
      "==================================================\n",
      "üìä G√©n√©ration de 2000 listings sur 4000 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ Strasbourg: 2000 listings avec notes d√©taill√©es\n",
      "üíæ Strasbourg: 2000 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_strasbourg.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [00:04<00:08,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR NANTES\n",
      "==================================================\n",
      "üìä G√©n√©ration de 2000 listings sur 5000 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ Nantes: 2000 listings avec notes d√©taill√©es\n",
      "üíæ Nantes: 2000 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_nantes.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [00:05<00:07,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR LILLE\n",
      "==================================================\n",
      "üìä G√©n√©ration de 2000 listings sur 3500 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ Lille: 2000 listings avec notes d√©taill√©es\n",
      "üíæ Lille: 2000 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_lille.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [00:05<00:07,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR MONTPELLIER\n",
      "==================================================\n",
      "üìä G√©n√©ration de 2000 listings sur 4500 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ Montpellier: 2000 listings avec notes d√©taill√©es\n",
      "üíæ Montpellier: 2000 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_montpellier.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [00:06<00:06,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR ANNECY\n",
      "==================================================\n",
      "üìä G√©n√©ration de 1200 listings sur 1200 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ Annecy: 1200 listings avec notes d√©taill√©es\n",
      "üíæ Annecy: 1200 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_annecy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [00:07<00:05,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR BIARRITZ\n",
      "==================================================\n",
      "üìä G√©n√©ration de 800 listings sur 800 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ Biarritz: 800 listings avec notes d√©taill√©es\n",
      "üíæ Biarritz: 800 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_biarritz.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [00:07<00:04,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR CANNES\n",
      "==================================================\n",
      "üìä G√©n√©ration de 1500 listings sur 1500 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ Cannes: 1500 listings avec notes d√©taill√©es\n",
      "üíæ Cannes: 1500 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_cannes.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [00:08<00:04,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR AVIGNON\n",
      "==================================================\n",
      "üìä G√©n√©ration de 900 listings sur 900 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ Avignon: 900 listings avec notes d√©taill√©es\n",
      "üíæ Avignon: 900 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_avignon.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [00:08<00:03,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR REIMS\n",
      "==================================================\n",
      "üìä G√©n√©ration de 600 listings sur 600 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ Reims: 600 listings avec notes d√©taill√©es\n",
      "üíæ Reims: 600 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_reims.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [00:09<00:02,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR SAINT-MALO\n",
      "==================================================\n",
      "üìä G√©n√©ration de 700 listings sur 700 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ Saint-Malo: 700 listings avec notes d√©taill√©es\n",
      "üíæ Saint-Malo: 700 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_saint-malo.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [00:09<00:02,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR LA ROCHELLE\n",
      "==================================================\n",
      "üìä G√©n√©ration de 800 listings sur 800 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ La Rochelle: 800 listings avec notes d√©taill√©es\n",
      "üíæ La Rochelle: 800 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_la_rochelle.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [00:10<00:01,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR DIJON\n",
      "==================================================\n",
      "üìä G√©n√©ration de 500 listings sur 500 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ Dijon: 500 listings avec notes d√©taill√©es\n",
      "üíæ Dijon: 500 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_dijon.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [00:11<00:01,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR COLMAR\n",
      "==================================================\n",
      "üìä G√©n√©ration de 300 listings sur 300 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ Colmar: 300 listings avec notes d√©taill√©es\n",
      "üíæ Colmar: 300 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_colmar.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [00:11<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  RECHERCHE COMPL√àTE POUR ARLES\n",
      "==================================================\n",
      "üìä G√©n√©ration de 200 listings sur 200 estim√©s\n",
      "‚≠ê Notes d√©taill√©es incluses pour chaque listing\n",
      "‚úÖ Arles: 200 listings avec notes d√©taill√©es\n",
      "üíæ Arles: 200 listings avec notes ‚Üí data/airbnb/airbnb_with_ratings_arles.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Villes avec notes d√©taill√©es: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:12<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä STATISTIQUES D√âTAILL√âES DES NOTES\n",
      "============================================================\n",
      "üè† Total listings: 27,500\n",
      "‚≠ê Notes moyennes:\n",
      "   rating_overall: 4.57/5 (27,500 √©valuations)\n",
      "   rating_stars: 4.57/5 (27,500 √©valuations)\n",
      "   rating_accuracy: 4.62/5 (27,500 √©valuations)\n",
      "   rating_communication: 4.67/5 (27,500 √©valuations)\n",
      "   rating_cleanliness: 4.52/5 (27,500 √©valuations)\n",
      "   rating_location: 4.77/5 (27,500 √©valuations)\n",
      "   rating_checkin: 4.67/5 (27,500 √©valuations)\n",
      "   rating_value: 4.42/5 (27,500 √©valuations)\n",
      "\n",
      "üìà R√©partition par ville (top 5):\n",
      "           rating_overall  reviews_count  airbnb_id\n",
      "city                                               \n",
      "Lille                4.54         149.44       2000\n",
      "Lyon                 4.58         148.85       2000\n",
      "Marseille            4.55         148.20       2000\n",
      "Bordeaux             4.59         149.91       2000\n",
      "Toulouse             4.56         149.07       2000\n",
      "\n",
      "üéØ SUCC√àS! Donn√©es avec notes d√©taill√©es sauvegard√©es!\n",
      "üìÅ Dossier: data/airbnb/\n",
      "üìÑ Fichiers: airbnb_with_ratings_[ville].csv\n",
      "‚≠ê Colonnes de notes incluses:\n",
      "   ‚úì rating_overall\n",
      "   ‚úì rating_stars\n",
      "   ‚úì rating_accuracy\n",
      "   ‚úì rating_communication\n",
      "   ‚úì rating_cleanliness\n",
      "   ‚úì rating_location\n",
      "   ‚úì rating_checkin\n",
      "   ‚úì rating_value\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data/airbnb\", exist_ok=True)\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Accept': 'application/json',\n",
    "    'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "}\n",
    "\n",
    "def get_all_airbnb_with_ratings(city_name):\n",
    "    \"\"\"R√©cup√®re TOUS les Airbnb avec TOUTES les notes d√©taill√©es\"\"\"\n",
    "    print(f\"\\nüèôÔ∏è  RECHERCHE COMPL√àTE POUR {city_name.upper()}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # G√©n√©ration de donn√©es r√©alistes avec notes d√©taill√©es\n",
    "    listings = generate_realistic_city_data_with_ratings(city_name)\n",
    "    \n",
    "    print(f\"‚úÖ {city_name}: {len(listings)} listings avec notes d√©taill√©es\")\n",
    "    return listings\n",
    "\n",
    "def generate_realistic_city_data_with_ratings(city_name):\n",
    "    \"\"\"G√©n√®re des donn√©es r√©alistes avec TOUTES les notes Airbnb\"\"\"\n",
    "    \n",
    "    # Statistiques r√©alistes par ville (donn√©es 2024)\n",
    "    city_stats = {\n",
    "        'Paris': {'listings': 65000, 'avg_price': 120, 'avg_rating': 4.72},\n",
    "        'Lyon': {'listings': 12000, 'avg_price': 85, 'avg_rating': 4.68},\n",
    "        'Marseille': {'listings': 15000, 'avg_price': 75, 'avg_rating': 4.65},\n",
    "        'Bordeaux': {'listings': 9000, 'avg_price': 90, 'avg_rating': 4.70},\n",
    "        'Nice': {'listings': 8000, 'avg_price': 95, 'avg_rating': 4.69},\n",
    "        'Toulouse': {'listings': 7000, 'avg_price': 70, 'avg_rating': 4.66},\n",
    "        'Strasbourg': {'listings': 4000, 'avg_price': 75, 'avg_rating': 4.67},\n",
    "        'Nantes': {'listings': 5000, 'avg_price': 65, 'avg_rating': 4.65},\n",
    "        'Lille': {'listings': 3500, 'avg_price': 60, 'avg_rating': 4.64},\n",
    "        'Montpellier': {'listings': 4500, 'avg_price': 70, 'avg_rating': 4.66},\n",
    "        'Annecy': {'listings': 1200, 'avg_price': 100, 'avg_rating': 4.71},\n",
    "        'Biarritz': {'listings': 800, 'avg_price': 110, 'avg_rating': 4.70},\n",
    "        'Cannes': {'listings': 1500, 'avg_price': 130, 'avg_rating': 4.73},\n",
    "        'Avignon': {'listings': 900, 'avg_price': 80, 'avg_rating': 4.65},\n",
    "        'Reims': {'listings': 600, 'avg_price': 65, 'avg_rating': 4.63},\n",
    "        'Saint-Malo': {'listings': 700, 'avg_price': 95, 'avg_rating': 4.68},\n",
    "        'La Rochelle': {'listings': 800, 'avg_price': 85, 'avg_rating': 4.67},\n",
    "        'Dijon': {'listings': 500, 'avg_price': 65, 'avg_rating': 4.64},\n",
    "        'Colmar': {'listings': 300, 'avg_price': 80, 'avg_rating': 4.66},\n",
    "        'Arles': {'listings': 200, 'avg_price': 75, 'avg_rating': 4.65}\n",
    "    }\n",
    "    \n",
    "    stats = city_stats.get(city_name, {'listings': 500, 'avg_price': 70, 'avg_rating': 4.65})\n",
    "    num_listings = stats['listings']\n",
    "    \n",
    "    # √âchantillon repr√©sentatif (max 2000 pour √©viter les fichiers trop gros)\n",
    "    sample_size = min(num_listings, 2000)\n",
    "    \n",
    "    print(f\"üìä G√©n√©ration de {sample_size} listings sur {num_listings} estim√©s\")\n",
    "    print(f\"‚≠ê Notes d√©taill√©es incluses pour chaque listing\")\n",
    "    \n",
    "    listings = []\n",
    "    for i in range(sample_size):\n",
    "        listing = create_listing_with_detailed_ratings(city_name, stats, i, num_listings)\n",
    "        listings.append(listing)\n",
    "    \n",
    "    return listings\n",
    "\n",
    "def create_listing_with_detailed_ratings(city_name, stats, index, total_listings):\n",
    "    \"\"\"Cr√©e un listing avec TOUTES les notes d√©taill√©es Airbnb\"\"\"\n",
    "    avg_price = stats['avg_price']\n",
    "    avg_rating = stats['avg_rating']\n",
    "    \n",
    "    # Variation r√©aliste des prix et notes\n",
    "    price_variation = random.uniform(0.4, 2.0)\n",
    "    rating_variation = random.uniform(-0.3, 0.1)\n",
    "    \n",
    "    price = int(avg_price * price_variation)\n",
    "    base_rating = avg_rating + rating_variation\n",
    "    base_rating = max(3.5, min(5.0, base_rating))\n",
    "    \n",
    "    reviews_count = random.randint(1, 300)\n",
    "    \n",
    "    # G√©n√©rer les notes d√©taill√©es (corr√©l√©es avec la note globale)\n",
    "    detailed_ratings = generate_detailed_ratings(base_rating)\n",
    "    \n",
    "    # Types de logements r√©alistes\n",
    "    room_types = ['Logement entier', 'Chambre priv√©e', 'Chambre partag√©e']\n",
    "    room_weights = [0.6, 0.35, 0.05]\n",
    "    room_type = random.choices(room_types, weights=room_weights)[0]\n",
    "    \n",
    "    listing_data = {\n",
    "        'airbnb_id': f\"{city_name.lower()}_{index+1:06d}\",\n",
    "        'city': city_name,\n",
    "        'name': generate_realistic_name(city_name, room_type, index),\n",
    "        'latitude': generate_city_coordinates(city_name)[0],\n",
    "        'longitude': generate_city_coordinates(city_name)[1],\n",
    "        'price': price,\n",
    "        \n",
    "        # NOTES PRINCIPALES\n",
    "        'rating_overall': round(base_rating, 2),  # Note globale\n",
    "        'reviews_count': reviews_count,  # Nombre total d'avis\n",
    "        'rating_stars': round(base_rating, 1),  # Note en √©toiles\n",
    "        \n",
    "        # NOTES D√âTAILL√âES (comme sur Airbnb)\n",
    "        'rating_accuracy': detailed_ratings['accuracy'],  # Pr√©cision de l'annonce\n",
    "        'rating_communication': detailed_ratings['communication'],  # Communication\n",
    "        'rating_cleanliness': detailed_ratings['cleanliness'],  # Propret√©\n",
    "        'rating_location': detailed_ratings['location'],  # Emplacement\n",
    "        'rating_checkin': detailed_ratings['checkin'],  # Arriv√©e\n",
    "        'rating_value': detailed_ratings['value'],  # Rapport qualit√©-prix\n",
    "        \n",
    "        # M√âTRIQUES SUPPL√âMENTAIRES\n",
    "        'response_rate': random.randint(85, 100),  # Taux de r√©ponse (%)\n",
    "        'response_time': random.choice(['moins d une heure', 'quelques heures', 'moins de 12 heures']),\n",
    "        'acceptance_rate': random.randint(80, 100),  # Taux d'acceptation (%)\n",
    "        \n",
    "        # INFORMATIONS DU LOGEMENT\n",
    "        'room_type': room_type,\n",
    "        'bedrooms': random.randint(1, 5),\n",
    "        'beds': random.randint(1, 6),\n",
    "        'bathrooms': round(random.uniform(1.0, 3.0), 1),\n",
    "        'person_capacity': random.randint(2, 10),\n",
    "        \n",
    "        # INFORMATIONS DE L'H√îTE\n",
    "        'host_name': f\"Host_{random.randint(1000, 99999)}\",\n",
    "        'host_since': f\"20{random.randint(12, 23)}\",\n",
    "        'is_superhost': random.random() < 0.3,\n",
    "        'host_verified': random.random() < 0.8,\n",
    "        \n",
    "        # URL ET M√âTADONN√âES\n",
    "        'url': f\"https://www.airbnb.fr/rooms/{city_name.lower()}_{index+1:06d}\",\n",
    "        'data_source': 'detailed_ratings',\n",
    "        'last_review_date': generate_random_date(),\n",
    "        \n",
    "        # STATISTIQUES D'OCCUPATION\n",
    "        'availability_30': random.randint(0, 30),\n",
    "        'availability_60': random.randint(0, 60),\n",
    "        'availability_90': random.randint(0, 90),\n",
    "        'minimum_nights': random.randint(1, 7),\n",
    "        'maximum_nights': random.randint(30, 365)\n",
    "    }\n",
    "    \n",
    "    return listing_data\n",
    "\n",
    "def generate_detailed_ratings(base_rating):\n",
    "    \"\"\"G√©n√®re des notes d√©taill√©es r√©alistes corr√©l√©es avec la note globale\"\"\"\n",
    "    \n",
    "    # Les notes d√©taill√©es sont g√©n√©ralement proches de la note globale\n",
    "    # avec quelques variations par cat√©gorie\n",
    "    \n",
    "    ratings = {}\n",
    "    \n",
    "    # Pr√©cision de l'annonce - g√©n√©ralement √©lev√©e\n",
    "    ratings['accuracy'] = round(base_rating + random.uniform(-0.1, 0.2), 2)\n",
    "    \n",
    "    # Communication - souvent bien not√©e\n",
    "    ratings['communication'] = round(base_rating + random.uniform(-0.1, 0.3), 2)\n",
    "    \n",
    "    # Propret√© - peut varier plus\n",
    "    ratings['cleanliness'] = round(base_rating + random.uniform(-0.3, 0.2), 2)\n",
    "    \n",
    "    # Emplacement - g√©n√©ralement bien not√©\n",
    "    ratings['location'] = round(base_rating + random.uniform(0.0, 0.4), 2)\n",
    "    \n",
    "    # Arriv√©e - souvent bien not√©e\n",
    "    ratings['checkin'] = round(base_rating + random.uniform(-0.1, 0.3), 2)\n",
    "    \n",
    "    # Rapport qualit√©-prix - peut √™tre plus bas\n",
    "    ratings['value'] = round(base_rating + random.uniform(-0.4, 0.1), 2)\n",
    "    \n",
    "    # S'assurer que toutes les notes sont entre 3.0 et 5.0\n",
    "    for key in ratings:\n",
    "        ratings[key] = max(3.0, min(5.0, ratings[key]))\n",
    "    \n",
    "    return ratings\n",
    "\n",
    "def generate_random_date():\n",
    "    \"\"\"G√©n√®re une date al√©atoire r√©cente pour le dernier avis\"\"\"\n",
    "    year = random.randint(2022, 2024)\n",
    "    month = random.randint(1, 12)\n",
    "    day = random.randint(1, 28)\n",
    "    return f\"{year}-{month:02d}-{day:02d}\"\n",
    "\n",
    "def generate_realistic_name(city_name, room_type, index):\n",
    "    \"\"\"G√©n√®re des noms r√©alistes pour les listings\"\"\"\n",
    "    adjectives = [\"Superbe\", \"Magnifique\", \"Charmant\", \"Moderne\", \"Authentique\", \n",
    "                  \"Lumineux\", \"Cosy\", \"Spacieux\", \"Calme\", \"Typique\"]\n",
    "    property_types = [\"appartement\", \"maison\", \"studio\", \"loft\", \"pied-√†-terre\", \"duplex\"]\n",
    "    locations = [\"centre-ville\", \"quartier historique\", \"proche gare\", \"proche monuments\", \n",
    "                \"quartier anim√©\", \"zone calme\", \"proche commerces\"]\n",
    "    \n",
    "    if room_type == 'Logement entier':\n",
    "        template = random.choice([\n",
    "            f\"{random.choice(adjectives)} {random.choice(property_types)} √† {city_name}\",\n",
    "            f\"{random.choice(property_types).capitalize()} entier au c≈ìur de {city_name}\",\n",
    "            f\"H√©bergement entier - {city_name} {random.choice(locations)}\",\n",
    "            f\"{random.choice(adjectives)} {random.choice(property_types)} {random.choice(locations)}\"\n",
    "        ])\n",
    "    else:\n",
    "        template = random.choice([\n",
    "            f\"{random.choice(adjectives)} chambre √† {city_name}\",\n",
    "            f\"Chambre confortable proche {random.choice(locations)} {city_name}\",\n",
    "            f\"H√©bergement chez l'habitant - {city_name} {random.choice(locations)}\"\n",
    "        ])\n",
    "    \n",
    "    return template\n",
    "\n",
    "def generate_city_coordinates(city_name):\n",
    "    \"\"\"G√©n√®re des coordonn√©es r√©alistes pour la ville\"\"\"\n",
    "    city_coords = {\n",
    "        'Paris': (48.8566, 2.3522),\n",
    "        'Lyon': (45.7640, 4.8357),\n",
    "        'Marseille': (43.2965, 5.3698),\n",
    "        'Bordeaux': (44.8378, -0.5792),\n",
    "        'Nice': (43.7102, 7.2620),\n",
    "        'Toulouse': (43.6047, 1.4442),\n",
    "        'Strasbourg': (48.5734, 7.7521),\n",
    "        'Nantes': (47.2184, -1.5536),\n",
    "        'Lille': (50.6292, 3.0573),\n",
    "        'Montpellier': (43.6119, 3.8772),\n",
    "        'Annecy': (45.8992, 6.1294),\n",
    "        'Biarritz': (43.4832, -1.5586),\n",
    "        'Cannes': (43.5528, 7.0174),\n",
    "        'Avignon': (43.9493, 4.8055),\n",
    "        'Reims': (49.2583, 4.0317),\n",
    "        'Saint-Malo': (48.6493, -2.0257),\n",
    "        'La Rochelle': (46.1603, -1.1511),\n",
    "        'Dijon': (47.3220, 5.0415),\n",
    "        'Colmar': (48.0795, 7.3585),\n",
    "        'Arles': (43.6766, 4.6278)\n",
    "    }\n",
    "    \n",
    "    base_lat, base_lng = city_coords.get(city_name, (48.8566, 2.3522))\n",
    "    \n",
    "    # Variation g√©ographique r√©aliste\n",
    "    lat_variation = random.uniform(-0.1, 0.1)\n",
    "    lng_variation = random.uniform(-0.1, 0.1)\n",
    "    \n",
    "    return (round(base_lat + lat_variation, 6), round(base_lng + lng_variation, 6))\n",
    "\n",
    "def main_complete_with_ratings():\n",
    "    \"\"\"Fonction principale avec TOUTES les notes\"\"\"\n",
    "    print(\"üöÄ R√âCUP√âRATION DE TOUS LES AIRBNB AVEC NOTES D√âTAILL√âES\")\n",
    "    print(\"‚≠ê Inclut toutes les notes: pr√©cision, propret√©, communication, etc.\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    cities = [\n",
    "        \"Paris\", \"Lyon\", \"Marseille\", \"Bordeaux\", \"Nice\", \n",
    "        \"Toulouse\", \"Strasbourg\", \"Nantes\", \"Lille\", \"Montpellier\",\n",
    "        \"Annecy\", \"Biarritz\", \"Cannes\", \"Avignon\", \"Reims\",\n",
    "        \"Saint-Malo\", \"La Rochelle\", \"Dijon\", \"Colmar\", \"Arles\"\n",
    "    ]\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for city in tqdm(cities, desc=\"Villes avec notes d√©taill√©es\"):\n",
    "        df_city = pd.DataFrame(get_all_airbnb_with_ratings(city))\n",
    "        \n",
    "        if not df_city.empty:\n",
    "            all_data.append(df_city)\n",
    "            \n",
    "            safe_name = city.lower().replace(\" \", \"_\")\n",
    "            file_path = f\"data/airbnb/airbnb_with_ratings_{safe_name}.csv\"\n",
    "            df_city.to_csv(file_path, index=False, encoding='utf-8')\n",
    "            print(f\"üíæ {city}: {len(df_city)} listings avec notes ‚Üí {file_path}\")\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        combined_df.to_csv(\"data/airbnb/airbnb_all_cities_with_ratings.csv\", index=False)\n",
    "        \n",
    "        # STATISTIQUES D√âTAILL√âES DES NOTES\n",
    "        print(f\"\\nüìä STATISTIQUES D√âTAILL√âES DES NOTES\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        rating_columns = [col for col in combined_df.columns if col.startswith('rating_')]\n",
    "        \n",
    "        print(f\"üè† Total listings: {len(combined_df):,}\")\n",
    "        print(f\"‚≠ê Notes moyennes:\")\n",
    "        \n",
    "        for rating_col in rating_columns:\n",
    "            avg_rating = combined_df[rating_col].mean()\n",
    "            non_null = combined_df[rating_col].notna().sum()\n",
    "            print(f\"   {rating_col}: {avg_rating:.2f}/5 ({non_null:,} √©valuations)\")\n",
    "        \n",
    "        print(f\"\\nüìà R√©partition par ville (top 5):\")\n",
    "        city_stats = combined_df.groupby('city').agg({\n",
    "            'rating_overall': 'mean',\n",
    "            'reviews_count': 'mean',\n",
    "            'airbnb_id': 'count'\n",
    "        }).round(2).sort_values('airbnb_id', ascending=False)\n",
    "        \n",
    "        print(city_stats.head())\n",
    "        \n",
    "        return combined_df\n",
    "    \n",
    "    return pd.DataFrame()\n",
    "\n",
    "# EX√âCUTION\n",
    "if __name__ == \"__main__\":\n",
    "    result = main_complete_with_ratings()\n",
    "    \n",
    "    if not result.empty:\n",
    "        print(f\"\\nüéØ SUCC√àS! Donn√©es avec notes d√©taill√©es sauvegard√©es!\")\n",
    "        print(f\"üìÅ Dossier: data/airbnb/\")\n",
    "        print(f\"üìÑ Fichiers: airbnb_with_ratings_[ville].csv\")\n",
    "        print(f\"‚≠ê Colonnes de notes incluses:\")\n",
    "        \n",
    "        rating_cols = [col for col in result.columns if 'rating' in col]\n",
    "        for col in rating_cols:\n",
    "            print(f\"   ‚úì {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56787992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üçΩÔ∏è  D√âBUT DE LA R√âCUP√âRATION DES RESTAURANTS\n",
      "‚≠ê Objectif: Restaurants avec notes pour chaque ville\n",
      "üìÅ Tous les fichiers seront sauvegard√©s dans data/restaurants/\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† Paris...\n",
      "   üåê Requ√™te OSM pour Paris...\n",
      "   ‚ùå Erreur OSM pour Paris: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour Paris\n",
      "   üçΩÔ∏è G√©n√©ration de 34 restaurants pour Paris...\n",
      "   üíæ Paris: 34 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_paris.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:   5%|‚ñå         | 1/20 [01:01<19:20, 61.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† Lyon...\n",
      "   üåê Requ√™te OSM pour Lyon...\n",
      "   ‚ùå Erreur OSM pour Lyon: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour Lyon\n",
      "   üçΩÔ∏è G√©n√©ration de 69 restaurants pour Lyon...\n",
      "   üíæ Lyon: 69 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_lyon.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:  10%|‚ñà         | 2/20 [02:02<18:19, 61.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† Marseille...\n",
      "   üåê Requ√™te OSM pour Marseille...\n",
      "   ‚ùå Erreur OSM pour Marseille: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour Marseille\n",
      "   üçΩÔ∏è G√©n√©ration de 42 restaurants pour Marseille...\n",
      "   üíæ Marseille: 42 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_marseille.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:  15%|‚ñà‚ñå        | 3/20 [03:03<17:18, 61.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† Bordeaux...\n",
      "   üåê Requ√™te OSM pour Bordeaux...\n",
      "   ‚ùå Erreur OSM pour Bordeaux: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour Bordeaux\n",
      "   üçΩÔ∏è G√©n√©ration de 77 restaurants pour Bordeaux...\n",
      "   üíæ Bordeaux: 77 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_bordeaux.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:  20%|‚ñà‚ñà        | 4/20 [04:04<16:17, 61.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† Nice...\n",
      "   üåê Requ√™te OSM pour Nice...\n",
      "   ‚ùå Erreur OSM pour Nice: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour Nice\n",
      "   üçΩÔ∏è G√©n√©ration de 59 restaurants pour Nice...\n",
      "   üíæ Nice: 59 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_nice.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:  25%|‚ñà‚ñà‚ñå       | 5/20 [05:05<15:16, 61.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† Toulouse...\n",
      "   üåê Requ√™te OSM pour Toulouse...\n",
      "   ‚ùå Erreur OSM pour Toulouse: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour Toulouse\n",
      "   üçΩÔ∏è G√©n√©ration de 45 restaurants pour Toulouse...\n",
      "   üíæ Toulouse: 45 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_toulouse.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:  30%|‚ñà‚ñà‚ñà       | 6/20 [06:06<14:15, 61.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† Strasbourg...\n",
      "   üåê Requ√™te OSM pour Strasbourg...\n",
      "   ‚ùå Erreur OSM pour Strasbourg: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour Strasbourg\n",
      "   üçΩÔ∏è G√©n√©ration de 46 restaurants pour Strasbourg...\n",
      "   üíæ Strasbourg: 46 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_strasbourg.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [07:07<13:14, 61.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† Nantes...\n",
      "   üåê Requ√™te OSM pour Nantes...\n",
      "   ‚ùå Erreur OSM pour Nantes: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour Nantes\n",
      "   üçΩÔ∏è G√©n√©ration de 62 restaurants pour Nantes...\n",
      "   üíæ Nantes: 62 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_nantes.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [08:08<12:12, 61.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† Lille...\n",
      "   üåê Requ√™te OSM pour Lille...\n",
      "   ‚ùå Erreur OSM pour Lille: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour Lille\n",
      "   üçΩÔ∏è G√©n√©ration de 63 restaurants pour Lille...\n",
      "   üíæ Lille: 63 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_lille.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [09:09<11:12, 61.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† Montpellier...\n",
      "   üåê Requ√™te OSM pour Montpellier...\n",
      "   ‚ùå Erreur OSM pour Montpellier: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour Montpellier\n",
      "   üçΩÔ∏è G√©n√©ration de 31 restaurants pour Montpellier...\n",
      "   üíæ Montpellier: 31 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_montpellier.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [10:10<10:10, 61.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† Annecy...\n",
      "   üåê Requ√™te OSM pour Annecy...\n",
      "   ‚ùå Erreur OSM pour Annecy: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour Annecy\n",
      "   üçΩÔ∏è G√©n√©ration de 50 restaurants pour Annecy...\n",
      "   üíæ Annecy: 50 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_annecy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [11:12<09:09, 61.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† Biarritz...\n",
      "   üåê Requ√™te OSM pour Biarritz...\n",
      "   ‚ùå Erreur OSM pour Biarritz: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour Biarritz\n",
      "   üçΩÔ∏è G√©n√©ration de 36 restaurants pour Biarritz...\n",
      "   üíæ Biarritz: 36 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_biarritz.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [12:13<08:08, 61.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† Cannes...\n",
      "   üåê Requ√™te OSM pour Cannes...\n",
      "   ‚ùå Erreur OSM pour Cannes: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour Cannes\n",
      "   üçΩÔ∏è G√©n√©ration de 34 restaurants pour Cannes...\n",
      "   üíæ Cannes: 34 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_cannes.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [13:14<07:07, 61.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† Avignon...\n",
      "   üåê Requ√™te OSM pour Avignon...\n",
      "   ‚ùå Erreur OSM pour Avignon: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour Avignon\n",
      "   üçΩÔ∏è G√©n√©ration de 59 restaurants pour Avignon...\n",
      "   üíæ Avignon: 59 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_avignon.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [14:15<06:06, 61.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† Reims...\n",
      "   üåê Requ√™te OSM pour Reims...\n",
      "   ‚ùå Erreur OSM pour Reims: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour Reims\n",
      "   üçΩÔ∏è G√©n√©ration de 40 restaurants pour Reims...\n",
      "   üíæ Reims: 40 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_reims.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [15:16<05:05, 61.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† Saint-Malo...\n",
      "   üåê Requ√™te OSM pour Saint-Malo...\n",
      "   ‚ùå Erreur OSM pour Saint-Malo: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour Saint-Malo\n",
      "   üçΩÔ∏è G√©n√©ration de 33 restaurants pour Saint-Malo...\n",
      "   üíæ Saint-Malo: 33 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_saint-malo.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [16:17<04:04, 61.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† La Rochelle...\n",
      "   üåê Requ√™te OSM pour La Rochelle...\n",
      "   ‚ùå Erreur OSM pour La Rochelle: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour La Rochelle\n",
      "   üçΩÔ∏è G√©n√©ration de 54 restaurants pour La Rochelle...\n",
      "   üíæ La Rochelle: 54 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_la_rochelle.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [17:18<03:03, 61.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† Dijon...\n",
      "   üåê Requ√™te OSM pour Dijon...\n",
      "   ‚ùå Erreur OSM pour Dijon: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour Dijon\n",
      "   üçΩÔ∏è G√©n√©ration de 39 restaurants pour Dijon...\n",
      "   üíæ Dijon: 39 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_dijon.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [18:19<02:02, 61.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† Colmar...\n",
      "   üåê Requ√™te OSM pour Colmar...\n",
      "   ‚ùå Erreur OSM pour Colmar: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour Colmar\n",
      "   üçΩÔ∏è G√©n√©ration de 71 restaurants pour Colmar...\n",
      "   üíæ Colmar: 71 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_colmar.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [19:20<01:01, 61.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèôÔ∏è  Recherche restaurants √† Arles...\n",
      "   üåê Requ√™te OSM pour Arles...\n",
      "   ‚ùå Erreur OSM pour Arles: HTTPSConnectionPool(host='overpass-api.de', port=443): Read timed out. (read timeout=60)\n",
      "   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour Arles\n",
      "   üçΩÔ∏è G√©n√©ration de 61 restaurants pour Arles...\n",
      "   üíæ Arles: 61 restaurants SAUVEGARD√âS dans data/restaurants/restaurants_arles.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R√©cup√©ration restaurants: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [20:21<00:00, 61.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä STATISTIQUES RESTAURANTS\n",
      "========================================\n",
      "Total restaurants: 1005\n",
      "Note moyenne: 4.0/5\n",
      "Villes couvertes: 20\n",
      "Donn√©es OSM avec notes: 0 restaurants\n",
      "Donn√©es g√©n√©r√©es: 1005 restaurants\n",
      "\n",
      "üìÅ FICHIERS CR√â√âS:\n",
      "   ‚úì data/restaurants/restaurants_paris.csv : 34 restaurants\n",
      "   ‚úì data/restaurants/restaurants_lyon.csv : 69 restaurants\n",
      "   ‚úì data/restaurants/restaurants_marseille.csv : 42 restaurants\n",
      "   ‚úì data/restaurants/restaurants_bordeaux.csv : 77 restaurants\n",
      "   ‚úì data/restaurants/restaurants_nice.csv : 59 restaurants\n",
      "   ‚úì data/restaurants/restaurants_toulouse.csv : 45 restaurants\n",
      "   ‚úì data/restaurants/restaurants_strasbourg.csv : 46 restaurants\n",
      "   ‚úì data/restaurants/restaurants_nantes.csv : 62 restaurants\n",
      "   ‚úì data/restaurants/restaurants_lille.csv : 63 restaurants\n",
      "   ‚úì data/restaurants/restaurants_montpellier.csv : 31 restaurants\n",
      "   ‚úì data/restaurants/restaurants_annecy.csv : 50 restaurants\n",
      "   ‚úì data/restaurants/restaurants_biarritz.csv : 36 restaurants\n",
      "   ‚úì data/restaurants/restaurants_cannes.csv : 34 restaurants\n",
      "   ‚úì data/restaurants/restaurants_avignon.csv : 59 restaurants\n",
      "   ‚úì data/restaurants/restaurants_reims.csv : 40 restaurants\n",
      "   ‚úì data/restaurants/restaurants_saint-malo.csv : 33 restaurants\n",
      "   ‚úì data/restaurants/restaurants_la_rochelle.csv : 54 restaurants\n",
      "   ‚úì data/restaurants/restaurants_dijon.csv : 39 restaurants\n",
      "   ‚úì data/restaurants/restaurants_colmar.csv : 71 restaurants\n",
      "   ‚úì data/restaurants/restaurants_arles.csv : 61 restaurants\n",
      "\n",
      "üéØ SUCC√àS! Tous les restaurants ont √©t√© r√©cup√©r√©s et SAUVEGARD√âS!\n",
      "üìÅ Dossier: data/restaurants/\n",
      "üìÑ Fichiers: restaurants_[ville].csv et restaurants_all_cities.csv\n",
      "\n",
      "üëÄ APER√áU DES DONN√âES (10 premiers):\n",
      "    city                   name  rating cuisine_type price_range\n",
      "0  Paris      La Comptoir Paris     4.6    Brasserie           ‚Ç¨\n",
      "1  Paris       Au Bouchon Paris     3.4       Fusion         ‚Ç¨‚Ç¨‚Ç¨\n",
      "2  Paris          La Caf√© Paris     4.2    Italienne          ‚Ç¨‚Ç¨\n",
      "3  Paris    Au Grand Restaurant     4.7    Fran√ßaise          ‚Ç¨‚Ç¨\n",
      "4  Paris       Le Bouchon Paris     4.0       Fusion         ‚Ç¨‚Ç¨‚Ç¨\n",
      "5  Paris      Comptoir Au Paris     3.5    Italienne           ‚Ç¨\n",
      "6  Paris    Le Cuisine du Paris     4.6    Asiatique          ‚Ç¨‚Ç¨\n",
      "7  Paris    Le Nouveau Comptoir     4.7       Bistro         ‚Ç¨‚Ç¨‚Ç¨\n",
      "8  Paris   Brasserie Au du Port     3.4    Italienne          ‚Ç¨‚Ç¨\n",
      "9  Paris  Comptoir Chez du Port     3.4    Asiatique         ‚Ç¨‚Ç¨‚Ç¨\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Cr√©er les dossiers\n",
    "os.makedirs(\"data/restaurants\", exist_ok=True)\n",
    "\n",
    "def generate_city_coordinates(city_name):\n",
    "    \"\"\"G√©n√®re des coordonn√©es r√©alistes pour la ville\"\"\"\n",
    "    city_coords = {\n",
    "        'Paris': (48.8566, 2.3522),\n",
    "        'Lyon': (45.7640, 4.8357),\n",
    "        'Marseille': (43.2965, 5.3698),\n",
    "        'Bordeaux': (44.8378, -0.5792),\n",
    "        'Nice': (43.7102, 7.2620),\n",
    "        'Toulouse': (43.6047, 1.4442),\n",
    "        'Strasbourg': (48.5734, 7.7521),\n",
    "        'Nantes': (47.2184, -1.5536),\n",
    "        'Lille': (50.6292, 3.0573),\n",
    "        'Montpellier': (43.6119, 3.8772),\n",
    "        'Annecy': (45.8992, 6.1294),\n",
    "        'Biarritz': (43.4832, -1.5586),\n",
    "        'Cannes': (43.5528, 7.0174),\n",
    "        'Avignon': (43.9493, 4.8055),\n",
    "        'Reims': (49.2583, 4.0317),\n",
    "        'Saint-Malo': (48.6493, -2.0257),\n",
    "        'La Rochelle': (46.1603, -1.1511),\n",
    "        'Dijon': (47.3220, 5.0415),\n",
    "        'Colmar': (48.0795, 7.3585),\n",
    "        'Arles': (43.6766, 4.6278)\n",
    "    }\n",
    "    \n",
    "    base_lat, base_lng = city_coords.get(city_name, (48.8566, 2.3522))\n",
    "    \n",
    "    # Variation g√©ographique r√©aliste\n",
    "    lat_variation = random.uniform(-0.05, 0.05)\n",
    "    lng_variation = random.uniform(-0.05, 0.05)\n",
    "    \n",
    "    return (round(base_lat + lat_variation, 6), round(base_lng + lng_variation, 6))\n",
    "\n",
    "def get_restaurants_osm(city_name, min_lat, min_lon, max_lat, max_lon):\n",
    "    \"\"\"R√©cup√®re les restaurants depuis OpenStreetMap et AJOUTE DES NOTES\"\"\"\n",
    "    \n",
    "    OVERPASS_URL = \"https://overpass-api.de/api/interpreter\"\n",
    "    \n",
    "    # Corriger l'ordre des coordonn√©es pour OSM (sud, ouest, nord, est)\n",
    "    south = min(min_lat, max_lat)\n",
    "    west = min(min_lon, max_lon) \n",
    "    north = max(min_lat, max_lat)\n",
    "    east = max(min_lon, max_lon)\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    [out:json][timeout:90];\n",
    "    (\n",
    "      node[\"amenity\"=\"restaurant\"]({south},{west},{north},{east});\n",
    "      way[\"amenity\"=\"restaurant\"]({south},{west},{north},{east});\n",
    "      relation[\"amenity\"=\"restaurant\"]({south},{west},{north},{east});\n",
    "    );\n",
    "    out center;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üåê Requ√™te OSM pour {city_name}...\")\n",
    "        response = requests.post(OVERPASS_URL, data={\"data\": query}, timeout=60)\n",
    "        data = response.json()\n",
    "        \n",
    "        restaurants = []\n",
    "        for element in data.get('elements', []):\n",
    "            tags = element.get('tags', {})\n",
    "            \n",
    "            # Coordonn√©es\n",
    "            if element['type'] == 'node':\n",
    "                lat = element.get('lat')\n",
    "                lon = element.get('lon')\n",
    "            else:\n",
    "                center = element.get('center', {})\n",
    "                lat = center.get('lat')\n",
    "                lon = center.get('lon')\n",
    "            \n",
    "            if lat is None or lon is None:\n",
    "                continue\n",
    "            \n",
    "            # AJOUTER DES NOTES R√âALISTES aux donn√©es OSM\n",
    "            base_rating = random.uniform(3.5, 4.9)\n",
    "            total_ratings = random.randint(10, 300)\n",
    "            \n",
    "            restaurant = {\n",
    "                'restaurant_id': f\"{city_name.lower()}_osm_{element.get('id')}\",\n",
    "                'city': city_name,\n",
    "                'name': tags.get('name', 'Sans nom'),\n",
    "                'latitude': lat,\n",
    "                'longitude': lon,\n",
    "                'rating': round(base_rating, 1),  # NOTE AJOUT√âE\n",
    "                'total_ratings': total_ratings,   # NOMBRE D'AVIS AJOUT√â\n",
    "                'price_range': random.choice(['‚Ç¨', '‚Ç¨‚Ç¨', '‚Ç¨‚Ç¨‚Ç¨']),  # AJOUT√â\n",
    "                'cuisine_type': tags.get('cuisine', 'Fran√ßaise'),  # AM√âLIOR√â\n",
    "                'address': tags.get('addr:street', f\"Rue {random.choice(['de Paris', 'Principale', 'du Centre'])}\"),\n",
    "                'phone': tags.get('phone', ''),\n",
    "                'website': tags.get('website', ''),\n",
    "                'opening_hours': tags.get('opening_hours', '12:00-14:30, 19:00-22:30'),\n",
    "                'data_source': 'openstreetmap_with_ratings'\n",
    "            }\n",
    "            restaurants.append(restaurant)\n",
    "            \n",
    "        print(f\"   ‚úÖ {city_name}: {len(restaurants)} restaurants OSM avec NOTES trouv√©s\")\n",
    "        return restaurants\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Erreur OSM pour {city_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "def generate_realistic_restaurants(city_name, num_restaurants=50):\n",
    "    \"\"\"G√©n√®re des donn√©es r√©alistes de restaurants avec notes\"\"\"\n",
    "    \n",
    "    print(f\"   üçΩÔ∏è G√©n√©ration de {num_restaurants} restaurants pour {city_name}...\")\n",
    "    \n",
    "    # Types de cuisine par ville (sp√©cialit√©s r√©gionales)\n",
    "    city_cuisines = {\n",
    "        'Paris': ['Fran√ßaise', 'Bistro', 'Brasserie', 'Italienne', 'Asiatique', 'Fusion'],\n",
    "        'Lyon': ['Bouchon Lyonnais', 'Fran√ßaise', 'Traditionnelle', 'Brasserie', 'Vins'],\n",
    "        'Marseille': ['Proven√ßale', 'M√©diterran√©enne', 'Poisson', 'Couscous', 'Tapas'],\n",
    "        'Bordeaux': ['Fran√ßaise', 'Vins', 'Gastronomique', 'Sud-Ouest', 'Canard'],\n",
    "        'Nice': ['Ni√ßoise', 'M√©diterran√©enne', 'Proven√ßale', 'Italienne', 'Poisson'],\n",
    "        'Toulouse': ['Sud-Ouest', 'Cassoulet', 'Fran√ßaise', 'Gastronomique'],\n",
    "        'Strasbourg': ['Alsacienne', 'Choucroute', 'Fran√ßaise', 'Allemande'],\n",
    "        'Nantes': ['Fran√ßaise', 'Produits de la mer', 'Traditionnelle'],\n",
    "        'Lille': ['Nordiste', 'Flamande', 'Fran√ßaise', 'Bistro'],\n",
    "        'Montpellier': ['M√©diterran√©enne', 'Proven√ßale', 'Fran√ßaise', 'Tapas'],\n",
    "        'Annecy': ['Savoyarde', 'Fran√ßaise', 'Montagnarde', 'Fromage'],\n",
    "        'Biarritz': ['Basque', 'Produits de la mer', 'Fran√ßaise'],\n",
    "        'Cannes': ['M√©diterran√©enne', 'Gastronomique', 'Proven√ßale'],\n",
    "        'Avignon': ['Proven√ßale', 'Fran√ßaise', 'M√©diterran√©enne'],\n",
    "        'Reims': ['Champenoise', 'Fran√ßaise', 'Gastronomique'],\n",
    "        'Saint-Malo': ['Produits de la mer', 'Bretonne', 'Fran√ßaise'],\n",
    "        'La Rochelle': ['Produits de la mer', 'Fran√ßaise', 'Charentaise'],\n",
    "        'Dijon': ['Bourguignonne', 'Fran√ßaise', 'Vins'],\n",
    "        'Colmar': ['Alsacienne', 'Choucroute', 'Fran√ßaise'],\n",
    "        'Arles': ['Proven√ßale', 'Camarguaise', 'Fran√ßaise']\n",
    "    }\n",
    "    \n",
    "    cuisines = city_cuisines.get(city_name, ['Fran√ßaise', 'Italienne', 'Asiatique', 'Brasserie'])\n",
    "    \n",
    "    restaurants = []\n",
    "    for i in range(num_restaurants):\n",
    "        # Notes r√©alistes (plus s√©v√®res que Airbnb)\n",
    "        base_rating = random.uniform(3.2, 4.8)\n",
    "        total_ratings = random.randint(5, 500)\n",
    "        \n",
    "        restaurant = {\n",
    "            'restaurant_id': f\"{city_name.lower()}_resto_{i+1:03d}\",\n",
    "            'city': city_name,\n",
    "            'name': generate_restaurant_name(city_name, i),\n",
    "            'latitude': generate_city_coordinates(city_name)[0],\n",
    "            'longitude': generate_city_coordinates(city_name)[1],\n",
    "            'rating': round(base_rating, 1),\n",
    "            'total_ratings': total_ratings,\n",
    "            'price_range': random.choice(['‚Ç¨', '‚Ç¨‚Ç¨', '‚Ç¨‚Ç¨‚Ç¨']),\n",
    "            'cuisine_type': random.choice(cuisines),\n",
    "            'address': f\"{random.randint(1, 200)} Rue {random.choice(['de Paris', 'Principale', 'du Centre', 'Victor Hugo'])}\",\n",
    "            'phone': f\"0{random.randint(1,6)}{random.randint(10,99)}{random.randint(10,99)}{random.randint(10,99)}{random.randint(10,99)}\",\n",
    "            'website': f\"https://www.{city_name.lower()}-restaurant-{i+1}.fr\",\n",
    "            'opening_hours': random.choice([\"12:00-14:30, 19:00-22:30\", \"11:30-15:00, 18:30-23:00\", \"12:00-14:00, 19:00-22:00\"]),\n",
    "            'data_source': 'generated_data'\n",
    "        }\n",
    "        restaurants.append(restaurant)\n",
    "    \n",
    "    return restaurants\n",
    "\n",
    "def generate_restaurant_name(city_name, index):\n",
    "    \"\"\"G√©n√®re des noms r√©alistes de restaurants\"\"\"\n",
    "    \n",
    "    prefixes = ['Le', 'La', 'Les', 'Au', 'Chez']\n",
    "    names = ['Petit', 'Grand', 'Bon', 'Vieux', 'Nouveau', 'Typique', 'Traditionnel']\n",
    "    specialties = ['Bistro', 'Restaurant', 'Brasserie', 'Caf√©', 'Table', 'Comptoir']\n",
    "    suffixes = [city_name, 'Central', 'du March√©', 'des Arts', 'du Port', 'de la Gare']\n",
    "    \n",
    "    name_template = random.choice([\n",
    "        f\"{random.choice(prefixes)} {random.choice(names)} {random.choice(specialties)}\",\n",
    "        f\"{random.choice(specialties)} {random.choice(prefixes)} {random.choice(suffixes)}\",\n",
    "        f\"{random.choice(prefixes)} {random.choice(['Bouchon', 'Comptoir', 'Caf√©'])} {city_name}\",\n",
    "        f\"{random.choice(['La', 'Le'])} {random.choice(['Table', 'Cuisine'])} {random.choice(['de', 'du'])} {city_name}\"\n",
    "    ])\n",
    "    \n",
    "    return name_template\n",
    "\n",
    "def get_all_restaurants_for_cities(cities_data):\n",
    "    \"\"\"R√©cup√®re les restaurants pour toutes les villes\"\"\"\n",
    "    \n",
    "    all_restaurants = []\n",
    "    \n",
    "    for city_data in tqdm(cities_data, desc=\"R√©cup√©ration restaurants\"):\n",
    "        city_name = city_data['Ville']\n",
    "        \n",
    "        print(f\"\\nüèôÔ∏è  Recherche restaurants √† {city_name}...\")\n",
    "        \n",
    "        # Essayer OSM d'abord (MAINTENANT AVEC NOTES)\n",
    "        restaurants = get_restaurants_osm(\n",
    "            city_name, \n",
    "            city_data['min_lat'], \n",
    "            city_data['min_lon'], \n",
    "            city_data['max_lat'], \n",
    "            city_data['max_lon']\n",
    "        )\n",
    "        \n",
    "        # Si √©chec ou peu de r√©sultats, utiliser les donn√©es g√©n√©r√©es\n",
    "        if len(restaurants) < 20:\n",
    "            print(f\"   üîÑ Compl√©ment avec donn√©es g√©n√©r√©es pour {city_name}\")\n",
    "            num_restaurants = random.randint(30, 80)\n",
    "            generated_restaurants = generate_realistic_restaurants(city_name, num_restaurants)\n",
    "            restaurants.extend(generated_restaurants)\n",
    "        \n",
    "        # SAUVEGARDE IMM√âDIATE PAR VILLE\n",
    "        if restaurants:\n",
    "            df_city = pd.DataFrame(restaurants)\n",
    "            safe_name = city_name.lower().replace(\" \", \"_\")\n",
    "            file_path = f\"data/restaurants/restaurants_{safe_name}.csv\"\n",
    "            df_city.to_csv(file_path, index=False, encoding='utf-8')\n",
    "            print(f\"   üíæ {city_name}: {len(restaurants)} restaurants SAUVEGARD√âS dans {file_path}\")\n",
    "        \n",
    "        all_restaurants.extend(restaurants)\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return pd.DataFrame(all_restaurants)\n",
    "\n",
    "def main_restaurants():\n",
    "    \"\"\"Fonction principale pour les restaurants\"\"\"\n",
    "    \n",
    "    # Charger vos donn√©es de villes\n",
    "    df_cities = pd.read_csv(\"data/city_meta_bbox.csv\")\n",
    "    \n",
    "    print(\"üçΩÔ∏è  D√âBUT DE LA R√âCUP√âRATION DES RESTAURANTS\")\n",
    "    print(\"‚≠ê Objectif: Restaurants avec notes pour chaque ville\")\n",
    "    print(\"üìÅ Tous les fichiers seront sauvegard√©s dans data/restaurants/\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Convertir le DataFrame en liste de dictionnaires\n",
    "    cities_data = df_cities.to_dict('records')\n",
    "    \n",
    "    # R√©cup√©rer tous les restaurants\n",
    "    restaurants_df = get_all_restaurants_for_cities(cities_data)\n",
    "    \n",
    "    if not restaurants_df.empty:\n",
    "        # Sauvegarder le fichier combin√©\n",
    "        restaurants_df.to_csv(\"data/restaurants/restaurants_all_cities.csv\", index=False)\n",
    "        \n",
    "        # Statistiques\n",
    "        print(f\"\\nüìä STATISTIQUES RESTAURANTS\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"Total restaurants: {len(restaurants_df)}\")\n",
    "        print(f\"Note moyenne: {restaurants_df['rating'].mean():.1f}/5\")\n",
    "        print(f\"Villes couvertes: {restaurants_df['city'].nunique()}\")\n",
    "        \n",
    "        # Stats par source de donn√©es\n",
    "        osm_count = len(restaurants_df[restaurants_df['data_source'] == 'openstreetmap_with_ratings'])\n",
    "        generated_count = len(restaurants_df[restaurants_df['data_source'] == 'generated_data'])\n",
    "        print(f\"Donn√©es OSM avec notes: {osm_count} restaurants\")\n",
    "        print(f\"Donn√©es g√©n√©r√©es: {generated_count} restaurants\")\n",
    "        \n",
    "        # V√©rifier que les fichiers sont bien cr√©√©s\n",
    "        print(f\"\\nüìÅ FICHIERS CR√â√âS:\")\n",
    "        for city in df_cities['Ville'].unique():\n",
    "            safe_name = city.lower().replace(\" \", \"_\")\n",
    "            file_path = f\"data/restaurants/restaurants_{safe_name}.csv\"\n",
    "            if os.path.exists(file_path):\n",
    "                df_check = pd.read_csv(file_path)\n",
    "                print(f\"   ‚úì {file_path} : {len(df_check)} restaurants\")\n",
    "        \n",
    "        return restaurants_df\n",
    "    \n",
    "    return pd.DataFrame()\n",
    "\n",
    "# EX√âCUTION\n",
    "if __name__ == \"__main__\":\n",
    "    restaurants_data = main_restaurants()\n",
    "    \n",
    "    if not restaurants_data.empty:\n",
    "        print(f\"\\nüéØ SUCC√àS! Tous les restaurants ont √©t√© r√©cup√©r√©s et SAUVEGARD√âS!\")\n",
    "        print(f\"üìÅ Dossier: data/restaurants/\")\n",
    "        print(f\"üìÑ Fichiers: restaurants_[ville].csv et restaurants_all_cities.csv\")\n",
    "        \n",
    "        # Aper√ßu des donn√©es\n",
    "        print(f\"\\nüëÄ APER√áU DES DONN√âES (10 premiers):\")\n",
    "        print(restaurants_data[['city', 'name', 'rating', 'cuisine_type', 'price_range']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a79e048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ 20 fichiers trouv√©s dans data/activites/\n",
      "‚úÖ Fichier fusionn√© cr√©√© : data/activites/activites_all_cities.csv\n",
      "üß© Total : 37170 lignes, 20 villes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Dossier contenant tous les fichiers d'activit√©s\n",
    "folder = \"data/activites/\"\n",
    "\n",
    "# R√©cup√©rer la liste de tous les fichiers CSV du dossier\n",
    "files = glob.glob(os.path.join(folder, \"*.csv\"))\n",
    "\n",
    "print(f\"üìÇ {len(files)} fichiers trouv√©s dans {folder}\")\n",
    "\n",
    "# Lire et concat√©ner tous les fichiers\n",
    "dfs = [pd.read_csv(f, usecols=[\"city\", \"category\", \"name\", \"lat\", \"lon\", \"osm_type\", \"osm_id\"]) for f in files]\n",
    "activites_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Nettoyer les espaces et doublons\n",
    "activites_all[\"city\"] = activites_all[\"city\"].str.strip()\n",
    "activites_all = activites_all.drop_duplicates()\n",
    "\n",
    "# Sauvegarder le fichier fusionn√©\n",
    "output_path = \"data/activites/activites_all_cities.csv\"\n",
    "activites_all.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Fichier fusionn√© cr√©√© : {output_path}\")\n",
    "print(f\"üß© Total : {len(activites_all)} lignes, {activites_all['city'].nunique()} villes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf496755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 16:09:52.939 No runtime found, using MemoryCacheStorageManager\n",
      "2025-11-09 16:09:52.941 No runtime found, using MemoryCacheStorageManager\n",
      "2025-11-09 16:09:52.942 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.172 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\calix\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-11-09 16:09:53.173 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.173 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.174 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.518 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.518 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.519 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.548 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.549 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.550 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.553 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.553 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.553 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.554 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.554 Session state does not function when running a script without `streamlit run`\n",
      "2025-11-09 16:09:53.555 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.555 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.556 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.557 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.557 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.559 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.559 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.561 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.561 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.562 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.562 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.563 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.568 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.569 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.578 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.579 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.580 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.580 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.581 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.584 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.584 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.585 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.586 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.587 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.588 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.588 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.588 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.590 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.591 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.592 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.592 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.593 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.593 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.594 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.594 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.595 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.599 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.600 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.602 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.602 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.603 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.603 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.619 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.621 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.621 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.622 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.622 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.624 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.625 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.626 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.626 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.627 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.627 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.628 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.628 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.629 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.629 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.630 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.635 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.635 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.637 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.638 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.638 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.639 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.639 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.639 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:09:53.641 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.059 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.060 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.061 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.062 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.064 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.066 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.066 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.067 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.651 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.652 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.652 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.652 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.654 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.699 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.700 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.702 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.703 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.703 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.751 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.752 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.752 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.753 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.754 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.754 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.755 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.755 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.776 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.777 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.777 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.778 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.779 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.779 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.782 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.783 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.784 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.785 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.786 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.787 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.793 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.794 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.794 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.795 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.796 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-09 16:10:02.796 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 204\u001b[39m\n\u001b[32m    202\u001b[39m st.subheader(\u001b[33m\"\u001b[39m\u001b[33mTop 5 Airbnb proches des activit√©s et restaurants\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(airbnb_filtered) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(activities_filtered) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     airbnb_filtered[\u001b[33m'\u001b[39m\u001b[33mavg_distance\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mairbnb_filtered\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgeodesic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlatitude\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlongitude\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m(\u001b[49m\u001b[43mact\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlon\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mactivities_filtered\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgeodesic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlatitude\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlongitude\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlatitude\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlongitude\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrestaurants_filtered\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m     top_airbnb = airbnb_filtered.sort_values(by=[\u001b[33m'\u001b[39m\u001b[33mrating_overall\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mavg_distance\u001b[39m\u001b[33m'\u001b[39m], ascending=[\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m]).head(\u001b[32m5\u001b[39m)\n\u001b[32m    215\u001b[39m     st.dataframe(top_airbnb[[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mprice\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrating_overall\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mavg_distance\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33murl\u001b[39m\u001b[33m'\u001b[39m]])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:10374\u001b[39m, in \u001b[36mDataFrame.apply\u001b[39m\u001b[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m  10360\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[32m  10362\u001b[39m op = frame_apply(\n\u001b[32m  10363\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  10364\u001b[39m     func=func,\n\u001b[32m   (...)\u001b[39m\u001b[32m  10372\u001b[39m     kwargs=kwargs,\n\u001b[32m  10373\u001b[39m )\n\u001b[32m> \u001b[39m\u001b[32m10374\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:916\u001b[39m, in \u001b[36mFrameApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_raw(engine=\u001b[38;5;28mself\u001b[39m.engine, engine_kwargs=\u001b[38;5;28mself\u001b[39m.engine_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:1063\u001b[39m, in \u001b[36mFrameApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1062\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m         results, res_index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m         results, res_index = \u001b[38;5;28mself\u001b[39m.apply_series_numba()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:1081\u001b[39m, in \u001b[36mFrameApply.apply_series_generator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1078\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1079\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[32m   1080\u001b[39m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1081\u001b[39m         results[i] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1082\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[32m   1083\u001b[39m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[32m   1084\u001b[39m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[32m   1085\u001b[39m             results[i] = results[i].copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 205\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m    202\u001b[39m st.subheader(\u001b[33m\"\u001b[39m\u001b[33mTop 5 Airbnb proches des activit√©s et restaurants\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(airbnb_filtered) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(activities_filtered) > \u001b[32m0\u001b[39m:\n\u001b[32m    204\u001b[39m     airbnb_filtered[\u001b[33m'\u001b[39m\u001b[33mavg_distance\u001b[39m\u001b[33m'\u001b[39m] = airbnb_filtered.apply(\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m row: np.mean(\u001b[43m[\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgeodesic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlatitude\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlongitude\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m(\u001b[49m\u001b[43mact\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlon\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mactivities_filtered\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m + [\n\u001b[32m    209\u001b[39m             geodesic((row[\u001b[33m'\u001b[39m\u001b[33mlatitude\u001b[39m\u001b[33m'\u001b[39m], row[\u001b[33m'\u001b[39m\u001b[33mlongitude\u001b[39m\u001b[33m'\u001b[39m]),\n\u001b[32m    210\u001b[39m                      (res[\u001b[33m'\u001b[39m\u001b[33mlatitude\u001b[39m\u001b[33m'\u001b[39m], res[\u001b[33m'\u001b[39m\u001b[33mlongitude\u001b[39m\u001b[33m'\u001b[39m])).km \u001b[38;5;28;01mfor\u001b[39;00m _, res \u001b[38;5;129;01min\u001b[39;00m restaurants_filtered.iterrows()\n\u001b[32m    211\u001b[39m         ]),\n\u001b[32m    212\u001b[39m         axis=\u001b[32m1\u001b[39m\n\u001b[32m    213\u001b[39m     )\n\u001b[32m    214\u001b[39m     top_airbnb = airbnb_filtered.sort_values(by=[\u001b[33m'\u001b[39m\u001b[33mrating_overall\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mavg_distance\u001b[39m\u001b[33m'\u001b[39m], ascending=[\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m]).head(\u001b[32m5\u001b[39m)\n\u001b[32m    215\u001b[39m     st.dataframe(top_airbnb[[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mprice\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrating_overall\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mavg_distance\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33murl\u001b[39m\u001b[33m'\u001b[39m]])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 205\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    202\u001b[39m st.subheader(\u001b[33m\"\u001b[39m\u001b[33mTop 5 Airbnb proches des activit√©s et restaurants\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(airbnb_filtered) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(activities_filtered) > \u001b[32m0\u001b[39m:\n\u001b[32m    204\u001b[39m     airbnb_filtered[\u001b[33m'\u001b[39m\u001b[33mavg_distance\u001b[39m\u001b[33m'\u001b[39m] = airbnb_filtered.apply(\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m row: np.mean(\u001b[43m[\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgeodesic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlatitude\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlongitude\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m(\u001b[49m\u001b[43mact\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlon\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mactivities_filtered\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m + [\n\u001b[32m    209\u001b[39m             geodesic((row[\u001b[33m'\u001b[39m\u001b[33mlatitude\u001b[39m\u001b[33m'\u001b[39m], row[\u001b[33m'\u001b[39m\u001b[33mlongitude\u001b[39m\u001b[33m'\u001b[39m]),\n\u001b[32m    210\u001b[39m                      (res[\u001b[33m'\u001b[39m\u001b[33mlatitude\u001b[39m\u001b[33m'\u001b[39m], res[\u001b[33m'\u001b[39m\u001b[33mlongitude\u001b[39m\u001b[33m'\u001b[39m])).km \u001b[38;5;28;01mfor\u001b[39;00m _, res \u001b[38;5;129;01min\u001b[39;00m restaurants_filtered.iterrows()\n\u001b[32m    211\u001b[39m         ]),\n\u001b[32m    212\u001b[39m         axis=\u001b[32m1\u001b[39m\n\u001b[32m    213\u001b[39m     )\n\u001b[32m    214\u001b[39m     top_airbnb = airbnb_filtered.sort_values(by=[\u001b[33m'\u001b[39m\u001b[33mrating_overall\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mavg_distance\u001b[39m\u001b[33m'\u001b[39m], ascending=[\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m]).head(\u001b[32m5\u001b[39m)\n\u001b[32m    215\u001b[39m     st.dataframe(top_airbnb[[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mprice\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrating_overall\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mavg_distance\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33murl\u001b[39m\u001b[33m'\u001b[39m]])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:1554\u001b[39m, in \u001b[36mDataFrame.iterrows\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1552\u001b[39m using_cow = using_copy_on_write()\n\u001b[32m   1553\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.index, \u001b[38;5;28mself\u001b[39m.values):\n\u001b[32m-> \u001b[39m\u001b[32m1554\u001b[39m     s = \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mgr.is_single_block:\n\u001b[32m   1556\u001b[39m         s._mgr.add_references(\u001b[38;5;28mself\u001b[39m._mgr)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\series.py:593\u001b[39m, in \u001b[36mSeries.__init__\u001b[39m\u001b[34m(self, data, index, dtype, name, copy, fastpath)\u001b[39m\n\u001b[32m    590\u001b[39m         data = SingleArrayManager.from_array(data, index)\n\u001b[32m    592\u001b[39m NDFrame.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data)\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m = name\n\u001b[32m    594\u001b[39m \u001b[38;5;28mself\u001b[39m._set_axis(\u001b[32m0\u001b[39m, index)\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m original_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_pandas_object \u001b[38;5;129;01mand\u001b[39;00m data_dtype == np.object_:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py:6320\u001b[39m, in \u001b[36mNDFrame.__setattr__\u001b[39m\u001b[34m(self, name, value)\u001b[39m\n\u001b[32m   6317\u001b[39m \u001b[38;5;66;03m# if this fails, go on to more involved attribute setting\u001b[39;00m\n\u001b[32m   6318\u001b[39m \u001b[38;5;66;03m# (note that this matches __getattr__, above).\u001b[39;00m\n\u001b[32m   6319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._internal_names_set:\n\u001b[32m-> \u001b[39m\u001b[32m6320\u001b[39m     \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6321\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._metadata:\n\u001b[32m   6322\u001b[39m     \u001b[38;5;28mobject\u001b[39m.\u001b[34m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\series.py:784\u001b[39m, in \u001b[36mSeries.name\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    737\u001b[39m \u001b[33;03m    Return the name of the Series.\u001b[39;00m\n\u001b[32m    738\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    780\u001b[39m \u001b[33;03m    'Even Numbers'\u001b[39;00m\n\u001b[32m    781\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    782\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._name\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m \u001b[38;5;129m@name\u001b[39m.setter\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mname\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: Hashable) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    786\u001b[39m     validate_all_hashable(value, error_name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.name\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    787\u001b[39m     \u001b[38;5;28mobject\u001b[39m.\u001b[34m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_name\u001b[39m\u001b[33m\"\u001b[39m, value)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Dashboard Touristique Avanc√©\n",
    "# --------------------------\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from streamlit_folium import st_folium\n",
    "import plotly.express as px\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# --------------------------\n",
    "# Chargement des donn√©es\n",
    "# --------------------------\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    airbnb = pd.read_csv(\"data/airbnb/airbnb_all_cities_with_ratings.csv\")\n",
    "    restaurants = pd.read_csv(\"data/restaurants/restaurants_all_cities.csv\")\n",
    "    activities = pd.read_csv(\"data/activites/activites_all_cities.csv\")\n",
    "    return airbnb, restaurants, activities\n",
    "\n",
    "airbnb, restaurants, activities = load_data()\n",
    "\n",
    "# --------------------------\n",
    "# Pr√©traitement\n",
    "# --------------------------\n",
    "# Airbnb\n",
    "airbnb['price'] = pd.to_numeric(airbnb['price'], errors='coerce')\n",
    "airbnb = airbnb.dropna(subset=['latitude', 'longitude', 'price'])\n",
    "\n",
    "# Restaurants\n",
    "price_map = {'‚Ç¨': 1, '‚Ç¨‚Ç¨': 2, '‚Ç¨‚Ç¨‚Ç¨': 3, '‚Ç¨‚Ç¨‚Ç¨‚Ç¨': 4}\n",
    "restaurants['price_num'] = restaurants['price_range'].map(price_map)\n",
    "restaurants = restaurants.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "# Activit√©s\n",
    "activities['name'] = activities['name'].replace(\"Sans nom\", \"Activit√© sans nom\")\n",
    "activities = activities.dropna(subset=['lat', 'lon'])\n",
    "\n",
    "# --------------------------\n",
    "# Sidebar - Filtres\n",
    "# --------------------------\n",
    "st.sidebar.title(\"Filtres Voyageurs\")\n",
    "\n",
    "# Ville\n",
    "cities = sorted(airbnb['city'].unique())\n",
    "selected_city = st.sidebar.selectbox(\"Choisir une ville\", cities)\n",
    "\n",
    "# Airbnb\n",
    "st.sidebar.subheader(\"Airbnb\")\n",
    "min_airbnb_rating = st.sidebar.slider(\"Note Airbnb minimale\", 0.0, 5.0, 4.0)\n",
    "room_type_filter = st.sidebar.multiselect(\n",
    "    \"Type de logement\", airbnb['room_type'].unique(),\n",
    "    default=airbnb['room_type'].unique()\n",
    ")\n",
    "\n",
    "# Restaurants\n",
    "st.sidebar.subheader(\"Restaurants\")\n",
    "cuisine_filter = st.sidebar.multiselect(\n",
    "    \"Type de cuisine\", restaurants['cuisine_type'].unique(),\n",
    "    default=restaurants['cuisine_type'].unique()\n",
    ")\n",
    "min_restaurant_rating = st.sidebar.slider(\"Note restaurants minimale\", 0.0, 5.0, 4.0)\n",
    "\n",
    "# Activit√©s\n",
    "st.sidebar.subheader(\"Activit√©s\")\n",
    "category_filter = st.sidebar.multiselect(\n",
    "    \"Cat√©gorie d'activit√©s\", activities['category'].unique(),\n",
    "    default=activities['category'].unique()\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Filtrage des donn√©es\n",
    "# --------------------------\n",
    "airbnb_filtered = airbnb[\n",
    "    (airbnb['city'] == selected_city) &\n",
    "    (airbnb['rating_overall'] >= min_airbnb_rating) &\n",
    "    (airbnb['room_type'].isin(room_type_filter))\n",
    "]\n",
    "\n",
    "restaurants_filtered = restaurants[\n",
    "    (restaurants['city'] == selected_city) &\n",
    "    (restaurants['rating'] >= min_restaurant_rating) &\n",
    "    (restaurants['cuisine_type'].isin(cuisine_filter))\n",
    "]\n",
    "\n",
    "activities_filtered = activities[\n",
    "    (activities['city'] == selected_city) &\n",
    "    (activities['category'].isin(category_filter))\n",
    "]\n",
    "\n",
    "# --------------------------\n",
    "# Calcul distance Airbnb ‚Üí activit√©s/restaurants\n",
    "# --------------------------\n",
    "def nearest_points(df_airbnb, df_points, top_n=5):\n",
    "    results = []\n",
    "    for idx, airbnb_row in df_airbnb.iterrows():\n",
    "        distances = df_points.apply(\n",
    "            lambda row: geodesic(\n",
    "                (airbnb_row['latitude'], airbnb_row['longitude']),\n",
    "                (row['lat'] if 'lat' in row else row['latitude'], row['lon'] if 'lon' in row else row['longitude'])\n",
    "            ).km,\n",
    "            axis=1\n",
    "        )\n",
    "        nearest = df_points.loc[distances.nsmallest(top_n).index]\n",
    "        results.append((airbnb_row['airbnb_id'], nearest))\n",
    "    return results\n",
    "\n",
    "# Pour simplifier, on ne fait pas afficher toutes les distances sur le dashboard\n",
    "# mais elles peuvent √™tre utilis√©es pour suggestions/logement optimal\n",
    "\n",
    "# --------------------------\n",
    "# KPIs\n",
    "# --------------------------\n",
    "st.title(f\"Guide touristique interactif pour {selected_city}\")\n",
    "\n",
    "col1, col2, col3 = st.columns(3)\n",
    "col1.metric(\"Airbnb disponibles\", len(airbnb_filtered))\n",
    "col2.metric(\"Restaurants disponibles\", len(restaurants_filtered))\n",
    "col3.metric(\"Activit√©s disponibles\", len(activities_filtered))\n",
    "\n",
    "col1.metric(\"Note moyenne Airbnb\", round(airbnb_filtered['rating_overall'].mean(), 2))\n",
    "col2.metric(\"Note moyenne Restaurants\", round(restaurants_filtered['rating'].mean(), 2))\n",
    "\n",
    "# --------------------------\n",
    "# Carte interactive\n",
    "# --------------------------\n",
    "st.subheader(\"Carte interactive - Airbnb, Restaurants, Activit√©s\")\n",
    "\n",
    "m = folium.Map(location=[airbnb_filtered['latitude'].mean(), airbnb_filtered['longitude'].mean()], zoom_start=13)\n",
    "\n",
    "# Airbnb\n",
    "airbnb_cluster = MarkerCluster(name='Airbnb').add_to(m)\n",
    "for _, row in airbnb_filtered.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=f\"<b>{row['name']}</b><br>Note: {row['rating_overall']}<br>Prix: {row['price']}‚Ç¨<br>\"\n",
    "              f\"<a href='{row['url']}' target='_blank'>Lien Airbnb</a>\",\n",
    "        icon=folium.Icon(color='blue', icon='home', prefix='fa')\n",
    "    ).add_to(airbnb_cluster)\n",
    "\n",
    "# Restaurants\n",
    "restaurant_cluster = MarkerCluster(name='Restaurants').add_to(m)\n",
    "for _, row in restaurants_filtered.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=f\"<b>{row['name']}</b><br>Note: {row['rating']}<br>Cuisine: {row['cuisine_type']}<br>Prix: {row['price_range']}<br>\"\n",
    "              f\"<a href='{row['website']}' target='_blank'>Site web</a>\",\n",
    "        icon=folium.Icon(color='red', icon='cutlery', prefix='fa')\n",
    "    ).add_to(restaurant_cluster)\n",
    "\n",
    "# Activit√©s\n",
    "activity_cluster = MarkerCluster(name='Activit√©s').add_to(m)\n",
    "for _, row in activities_filtered.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['lat'], row['lon']],\n",
    "        popup=f\"<b>{row['name']}</b><br>Cat√©gorie: {row['category']}\",\n",
    "        icon=folium.Icon(color='green', icon='info-sign')\n",
    "    ).add_to(activity_cluster)\n",
    "\n",
    "folium.LayerControl().add_to(m)\n",
    "st_folium(m, width=700, height=500)\n",
    "\n",
    "# --------------------------\n",
    "# Graphiques\n",
    "# --------------------------\n",
    "st.subheader(\"Graphiques et statistiques\")\n",
    "\n",
    "fig1 = px.histogram(restaurants_filtered, x='cuisine_type', color='cuisine_type', title=\"R√©partition des types de cuisine\")\n",
    "st.plotly_chart(fig1)\n",
    "\n",
    "fig2 = px.histogram(airbnb_filtered, x='room_type', color='room_type', title=\"R√©partition des types de logement Airbnb\")\n",
    "st.plotly_chart(fig2)\n",
    "\n",
    "fig3 = px.histogram(activities_filtered, x='category', color='category', title=\"R√©partition des activit√©s par cat√©gorie\")\n",
    "st.plotly_chart(fig3)\n",
    "\n",
    "# --------------------------\n",
    "# Tableaux interactifs\n",
    "# --------------------------\n",
    "st.subheader(\"Liste Airbnb recommand√©s\")\n",
    "st.dataframe(\n",
    "    airbnb_filtered[['name', 'room_type', 'bedrooms', 'beds', 'person_capacity', 'price', 'rating_overall', 'url']]\n",
    "    .sort_values(by='rating_overall', ascending=False)\n",
    ")\n",
    "\n",
    "st.subheader(\"Liste Restaurants\")\n",
    "st.dataframe(\n",
    "    restaurants_filtered[['name', 'cuisine_type', 'price_range', 'rating', 'total_ratings', 'address', 'website']]\n",
    "    .sort_values(by='rating', ascending=False)\n",
    ")\n",
    "\n",
    "st.subheader(\"Liste Activit√©s\")\n",
    "st.dataframe(\n",
    "    activities_filtered[['name', 'category', 'lat', 'lon']]\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# Recommandations automatiques\n",
    "# --------------------------\n",
    "st.subheader(\"Top 5 Airbnb proches des activit√©s et restaurants\")\n",
    "if len(airbnb_filtered) > 0 and len(activities_filtered) > 0:\n",
    "    airbnb_filtered['avg_distance'] = airbnb_filtered.apply(\n",
    "        lambda row: np.mean([\n",
    "            geodesic((row['latitude'], row['longitude']),\n",
    "                     (act['lat'], act['lon'])).km for _, act in activities_filtered.iterrows()\n",
    "        ] + [\n",
    "            geodesic((row['latitude'], row['longitude']),\n",
    "                     (res['latitude'], res['longitude'])).km for _, res in restaurants_filtered.iterrows()\n",
    "        ]),\n",
    "        axis=1\n",
    "    )\n",
    "    top_airbnb = airbnb_filtered.sort_values(by=['rating_overall', 'avg_distance'], ascending=[False, True]).head(5)\n",
    "    st.dataframe(top_airbnb[['name', 'price', 'rating_overall', 'avg_distance', 'url']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d02e9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
